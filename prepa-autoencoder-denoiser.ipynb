{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls ${CUDA_DIR}/nvvm/libdevice\n",
    "# !set MLIR_CRASH_REPRODUCER_DIRECTORY=\"enable\"\n",
    "# !set LD_LIBRARY_PATH=\"~/.local/lib/python3.10/site-packages/tensorrt\"\n",
    "# !ls ~/.local/lib/python3.10/site-packages/tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"12\"\n",
    "# os.environ[\"LD_LIBRARY_PATH\"]=\"~/.local/lib/python3.10/site-packages/tensorflow\"\n",
    "# os.environ[\"LD_LIBRARY_PATH\"]=\"/home/pedro/anaconda3/envs/tensorflow/lib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia_base = \"~/.local/lib/python3.10.site-packages/nvidia\"\n",
    "\n",
    "# nvidia_paths = [\"cusparse\", \"cusolver\", \"cuda_runtime\"]\n",
    "\n",
    "# nvidia_libs = []\n",
    "# for np in nvidia_paths:\n",
    "#     nvidia_libs.append(f\"{nvidia_base}/{np}/lib\")\n",
    "\n",
    "\n",
    "# cuda = [\"/usr/lib/x86_64-linux-gnu\",\"/usr/local/cuda-12.1/lib64\", \"~/.local/lib/python3.10/site-packages/tensorrt\", *nvidia_libs]\n",
    "\n",
    "# ld_library_path = \":\".join(cuda)\n",
    "\n",
    "# os.environ[\"LD_LIBRARY_PATH\"] = ld_library_path\n",
    "# ld_library_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext filprofiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import math\n",
    "from json import dumps\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from IPython import display\n",
    "import tensorflow_addons as tfa\n",
    "# for g in gpus:\n",
    "#     tf.config.experimental.set_memory_growth(g, True)\n",
    "    \n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten,\\\n",
    "                                    Reshape, LeakyReLU as LR,\\\n",
    "                                    Activation, Dropout\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50V2 as ResNet\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.layers import Conv2D, LeakyReLU, MaxPooling2D, Rescaling\n",
    "from tensorflow.keras.regularizers import L2\n",
    "from enum import Enum\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(f\"gpus={gpus}\")\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(8) \n",
    "tf.config.threading.set_intra_op_parallelism_threads(8)\n",
    "# if gpus:\n",
    "#     try:\n",
    "#         total_gpu_memory = 23817\n",
    "#         tf.config.experimental.set_virtual_device_configuration(gpus[0],\n",
    "#             [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=total_gpu_memory-1000)])\n",
    "#     except RuntimeError as e:\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TensorboardUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "from functools import reduce\n",
    "splits = tfds.even_splits('train', n=200, drop_remainder=True)\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def now():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime('%Y_%m_%d_T%H_%M_%S') + ('_%02d' % (now.microsecond / 10000))\n",
    "\n",
    "def minute():\n",
    "    now = datetime.datetime.now()\n",
    "    return now.strftime('%H_%M') \n",
    "\n",
    "minute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "class LossNetwork(tf.keras.models.Model):\n",
    "    def __init__(self):\n",
    "        super(LossNetwork, self).__init__()\n",
    "        vgg = VGG16(include_top=False, weights='imagenet')\n",
    "        vgg.trainable = False\n",
    "        \n",
    "        self.selected_layers = ['block1_conv1', 'block2_conv2',\"block3_conv3\" ,'block4_conv3','block5_conv3']\n",
    "        self.selected_layer_weights = [0.1, 0.4 , 0.4 , 0.8 , 1.6]\n",
    "        \n",
    "        model_outputs = [vgg.get_layer(name).output for name in self.selected_layers]\n",
    "        self.model = tf.keras.models.Model(vgg.input, model_outputs)\n",
    "        # mixed precision float32 output\n",
    "        self.linear = layers.Activation('linear', dtype='float32') \n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        x = preprocess_input(x)\n",
    "        x = self.model(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "#     @tf.function\n",
    "    def loss(self, x, y):\n",
    "        h1_list = self.model(x)\n",
    "        h2_list = self.model(y)\n",
    "        \n",
    "        rc_loss = 0.0\n",
    "        \n",
    "        for h1, h2, weight in zip(h1_list, h2_list, self.selected_layer_weights):\n",
    "            h1 = tf.cast(h1, tf.float32)\n",
    "            h2 = tf.cast(h2, tf.float32)\n",
    "            h1 = K.batch_flatten(h1)\n",
    "            h2 = K.batch_flatten(h2)\n",
    "            rc_loss = rc_loss + tf.cast(tf.reduce_mean((h1 - h2)**2), tf.float32) * weight   \n",
    "            rc_loss = tf.cast(rc_loss, tf.float32)\n",
    "      \n",
    "        return rc_loss\n",
    "    \n",
    "\n",
    "    def loss2(self, x, y):\n",
    "        h1_list = self.model(x)\n",
    "        h2_list = self.model(y)\n",
    "        \n",
    "        rc_loss = 0.0\n",
    "        \n",
    "        for h1, h2, weight in zip(h1_list, h2_list, self.selected_layer_weights):\n",
    "            h1 = tf.cast(h1, tf.float32)\n",
    "            h2 = tf.cast(h2, tf.float32)\n",
    "\n",
    "            h1 = h1 / 100\n",
    "            h2 = h2 / 100\n",
    "#             h1 = K.batch_flatten(h1)\n",
    "#             h2 = K.batch_flatten(h2)\n",
    "            rc_loss = rc_loss + tf.cast(tf.reduce_mean((h1 - h2)**2, axis=[1,2,3]), tf.float32) * weight   \n",
    "            rc_loss = tf.cast(rc_loss, tf.float32)\n",
    "      \n",
    "        return rc_loss\n",
    "    \n",
    "class PeceptualLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self):\n",
    "        super(PeceptualLoss, self).__init__()\n",
    "        self.loss_network = LossNetwork()\n",
    "        self.loss_network.trainable = False\n",
    "        \n",
    "    def call(self, y_true, y_pred):\n",
    "        return self.loss_network.loss(y_true,y_pred)\n",
    "\n",
    "    \n",
    "def build_perceptual_loss(*args, **kwargs):\n",
    "    return PeceptualLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import ImageRenderer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import CustomTrainStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best result yet:\n",
    "\n",
    "- In 150/300 epochs, with ranges best_lr_range = [3e-5, 5e-7] and linspace of size 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(layer_number, name):\n",
    "    def fn(model):\n",
    "        model.layers[layer_number].save_weights(name)\n",
    "        print(f\"Weights for layer {model.layers[layer_number].name} saved at {name}\")\n",
    "    return fn\n",
    "\n",
    "def save_layers(layer_names, names):\n",
    "    def fn(model):\n",
    "        for layer, name in zip(list(filter(lambda layer: layer.name in layer_names, model.layers)), names):\n",
    "            layer.save_weights(name)\n",
    "            print(f\"Weights for layer {layer.name} saved at {name}\")\n",
    "    return fn\n",
    "\n",
    "def save_weights_from_map(name_map):\n",
    "    def fn(model):\n",
    "        print(\"Saving weights\")\n",
    "        model_map = extract_models(model)\n",
    "        for key, value in model_map.items():\n",
    "            print(f\"Saving {value.name} to {name_map[key]}\")\n",
    "            value.save_weights(name_map[key])\n",
    "        print(f\"Saved weights\")\n",
    "        \n",
    "    return fn\n",
    "\n",
    "def save_multiple_weights(layer_numbers_with_names):\n",
    "    def fn(model):\n",
    "        for layer_number, name in layer_numbers:\n",
    "            model.layers[layer_number].save_weights(name)\n",
    "            print(f\"Weights for layer {model.layers[layer_number].name} saved at {name}\")\n",
    "    return fn\n",
    "\n",
    "# save_layers([\"decoder_small\"], ['./weights/decoder_small'])(smodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ELU https://deeplearninguniversity.com/elu-as-an-activation-function-in-neural-networks/: Allowed to get more details through the network without connecting layers from the decoder to layers in the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def resize_input(inputs, output_shape=(120, 176), rescale_input=True):\n",
    "    oh = output_shape[0]\n",
    "    ow = output_shape[1]\n",
    "    y = tf.keras.layers.Rescaling(1./255)(inputs) if rescale_input else inputs\n",
    "    y = tf.cast(y, tf.float16)\n",
    "    y = tf.image.resize(\n",
    "        y,\n",
    "        [oh,ow],\n",
    "        preserve_aspect_ratio=False,\n",
    "        antialias=False,\n",
    "        name=None)\n",
    "    return y\n",
    "\n",
    "class DecoderSize(Enum):\n",
    "    xs = (60, 90)\n",
    "    sm = (240, 360)\n",
    "    md = (480, 720)\n",
    "    lg = (720, 1080)\n",
    "    \n",
    "class CustomUpSampling2D(tf.keras.layers.Layer):\n",
    "  def __init__(self, size):\n",
    "    super(CustomUpSampling2D, self).__init__()\n",
    "    if type(size) is not tuple and type(size) is not list:\n",
    "        size = (size, size)\n",
    "    self.size = size\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    pass\n",
    "\n",
    "  def call(self, input):\n",
    "      return tf.repeat(tf.repeat(input, self.size[0], axis=1), self.size[1], axis=2)\n",
    "        \n",
    "class LayerFactory:\n",
    "    def __init__(self):\n",
    "        self.downs = 0\n",
    "        self.ups = 0\n",
    "        self.resbs = 0\n",
    "        \n",
    "    def get_up_name(self):\n",
    "        self.ups += 1\n",
    "        return self.ups\n",
    "    \n",
    "    def get_down_name(self):\n",
    "        self.downs += 1\n",
    "        return self.downs\n",
    "    \n",
    "    def get_resb_name(self, name=None):\n",
    "        if name is None:\n",
    "            self.resbs+=1\n",
    "            return self.resbs\n",
    "        return name\n",
    "        \n",
    "        \n",
    "    def conv(self, filters, kernel_size=3, strides=1, activation=\"elu\", norm=True, **args):\n",
    "        def fn(x):\n",
    "            x = layers.Conv2D(filters,\n",
    "                             kernel_size=kernel_size,\n",
    "                             strides=strides,\n",
    "                             padding=\"same\",\n",
    "                             use_bias=True,\n",
    "#                              activation=activation,\n",
    "#                              kernel_regularizer=L2(1e-2),\n",
    "                             **args)(x)\n",
    "#             if norm:\n",
    "#                 x = layers.BatchNormalization()(x)\n",
    "            return layers.Activation(activation)(x)\n",
    "        return fn\n",
    "    \n",
    "    def down(self, filters, kernel_size=3,strides=2):\n",
    "        def fn(x):\n",
    "            x = self.conv(filters, kernel_size=kernel_size, strides=2, norm=True)(x)\n",
    "            return self.resblock(filters, kernel_size, name=f\"fdown_{self.get_down_name()}\", norm=True)(x)\n",
    "        return fn\n",
    "        \n",
    "    def convblock(self, filters, kernel_size=3, name=None):\n",
    "        def fn(x, ff=None):\n",
    "            return self.resblock(filters, kernel_size)(x)\n",
    "        return fn\n",
    "\n",
    "    def resblock(self, filters, kernel_size=3, activation=\"elu\", name=None, norm=True):\n",
    "        def fn(x):\n",
    "            resbname = self.get_resb_name(name)\n",
    "            x = self.conv(filters, activation=activation, norm=norm, name=f\"{resbname}_resb_conv_1\")(x)\n",
    "            y = self.conv(filters, activation=activation, norm=norm, name=f\"{resbname}_resb_conv_2\")(x)\n",
    "            y = self.conv(filters, activation=activation, norm=norm, name=f\"{resbname}_resb_conv_3\")(y)\n",
    "            out = layers.Add()([x,y]) / 2\n",
    "#             out = layers.BatchNormalization()(out)\n",
    "            return tf.keras.activations.elu(out)\n",
    "        return fn\n",
    "    \n",
    "    \n",
    "    def up(self, filters, kernel_size=3):\n",
    "        def fn(x):\n",
    "#             x = tf.stop_gradient(tf.keras.layers.UpSampling2D(size=(2, 2))(x))\n",
    "#             x = CustomUpSampling2D(size=(2, 2))(x)\n",
    "            x = tf.keras.layers.UpSampling2D(size=(2, 2))(x)\n",
    "\n",
    "#             x = upsample_no_grads(x)\n",
    "#             x = self.conv(filters, kernel_size=kernel_size, strides=1, norm=False)(x)\n",
    "            x = self.resblock(filters, kernel_size, name=f\"up_{self.get_up_name()}\", norm=False)(x)\n",
    "            return x\n",
    "        return fn\n",
    "    \n",
    "\n",
    "class MyRescale(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(MyRescale, self).__init__()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean = 255./2\n",
    "        dev = 255\n",
    "        return  (inputs - mean) / dev\n",
    "    \n",
    "    \n",
    "def build_small_decoder(decoder_size=DecoderSize.xs):\n",
    "    l = LayerFactory()\n",
    "    \n",
    "    inputs0 = keras.Input(shape=(480, 720, 3), dtype=tf.float16, name=\"autoencoder_input\")\n",
    "    i1 = MyRescale()(inputs0)\n",
    "\n",
    "    s1 = l.resblock(64,kernel_size=3, norm=False)(i1)\n",
    "    s2 = l.resblock(64,kernel_size=5, norm=False)(i1)\n",
    "    \n",
    "    x = tf.add(s1,s2) / 2\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = tf.keras.activations.elu(x)\n",
    "   \n",
    "    x = l.down(128)(x)\n",
    "    x = l.down(256)(x)\n",
    "#     x = layers.SpatialDropout2D(0.1)(x)\n",
    "    x = l.down(512)(x)\n",
    "    x = layers.SpatialDropout2D(0.1)(x)\n",
    "    x = l.down(512)(x)\n",
    "    x = layers.SpatialDropout2D(0.1)(x)\n",
    "\n",
    "    x = l.up(512)(x)    \n",
    "    x = l.up(512)(x)\n",
    "    x = l.up(128)(x)\n",
    "    x = l.up(64)(x)\n",
    "    x = l.resblock(32 ,3, norm=False)(x)\n",
    "    x = l.resblock(9 ,3, norm=False)(x)\n",
    "    x = l.conv(3, 3, activation=\"sigmoid\", norm=False)(x)\n",
    "    x = tf.keras.layers.Rescaling(255.)(x)\n",
    "    x = layers.Activation(\"linear\", dtype=tf.float32)(x)\n",
    "\n",
    "    return tf.keras.Model(inputs0, x, name=\"decoder_small\")\n",
    "\n",
    "\n",
    "\n",
    "model = build_small_decoder()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras import backend as K\n",
    "model.layers[2].weights, sys.getsizeof(model.layers[2].weights)\n",
    "\n",
    "# [K.batch_flatten(l) for l in model.layers[2:]]\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "\n",
    "def compare_model_weights(m1, m2):\n",
    "    ws = []\n",
    "    for i, l1 in enumerate(m1.layers):\n",
    "        l2 = m2.layers[i]\n",
    "        if len(l1.weights) > 0:\n",
    "            for wi, w1 in enumerate(l1.weights):\n",
    "                w1 = K.batch_flatten(l1.weights[wi])\n",
    "                w2 = K.batch_flatten(l2.weights[wi])\n",
    "#             print(l1.weights[2].shape, l2.weights[2].shape)\n",
    "                ws.append(mae(w1, w2))\n",
    "    return tf.math.reduce_sum(ws)\n",
    "\n",
    "# model.load_weights(\"weights/denoiser_good\")\n",
    "# mse(get_model_weights(build_small_decoder()), get_model_weights(model))\n",
    "\n",
    "\n",
    "class GoodModelMetric(tf.keras.metrics.Metric):\n",
    "    def __init__(self, other_model, **kwargs):\n",
    "        super(GoodModelMetric, self).__init__(name=\"GoodModelMetric\", **kwargs)\n",
    "        \n",
    "        self.good_model = build_small_decoder()\n",
    "        self.good_model.load_weights(\"weights/denoiser_good\")\n",
    "        self.sim = self.add_weight(name='similarity', initializer='zeros')\n",
    "        self.other_model = other_model\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.sim.assign(compare_model_weights(self.good_model, self.other_model))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sim\n",
    "\n",
    "# wm1 = compare_model_weights(model, build_small_decoder())\n",
    "# # np.pad(wm1, pad_width=((0, 0), (1, 1)), mode='constant')\n",
    "# wm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/denoiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(lr=1e-4):\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#         optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "        return optimizer\n",
    "    \n",
    "def can_save(h):\n",
    "    has_history = len(h.history[\"val_loss\"]) > 1\n",
    "#     improved = h.history[\"val_loss\"][0] < h.history[\"val_loss\"][-1]\n",
    "    not_nan = not tf.math.is_nan(h.history[\"val_loss\"][-1])\n",
    "    return has_history and not_nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageMetric(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, name, **kwargs):\n",
    "        super(ImageMetric, self).__init__(name=name, **kwargs)\n",
    "        self.sim = self.add_weight(name='similarity', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        fn = lambda x,y: 0\n",
    "        if self.name == \"ssim_multiscale\":\n",
    "            fn = lambda x,y: tf.image.ssim_multiscale(x,y,255)\n",
    "        values =fn(y_true,y_pred)\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "            sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "            values = tf.multiply(values, sample_weight)\n",
    "        self.sim.assign(tf.reduce_mean(values))\n",
    "\n",
    "    def result(self):\n",
    "        return self.sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRMetric(tf.keras.metrics.Metric):\n",
    "\n",
    "    def __init__(self, optimizer, name=\"learning_rate\", **kwargs):\n",
    "        super(LRMetric, self).__init__(name, **kwargs)\n",
    "        self.optimizer = optimizer\n",
    "        self.lr = self.add_weight(name='lr', initializer='zeros')\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.lr.assign(self.optimizer.lr)\n",
    "\n",
    "    def result(self):\n",
    "        return self.lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoder_tools.ae_data import div2k_ds\n",
    "\n",
    "dec_size = DecoderSize.md\n",
    "batch_size = 4\n",
    "train_ds = div2k_ds(\"train\", dec_size.value, batch_size=batch_size)\n",
    "test_ds = div2k_ds(\"validation\", dec_size.value, batch_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RenderCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, tbutil):\n",
    "        self.images_renderer = ImageRenderer(3).withTensorboard(tbutil)\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        def ren(m):\n",
    "            self.images_renderer.render(model=self.model, dataset=test_ds, batch=0)\n",
    "            return 1\n",
    "        tf.numpy_function(ren, (1,), Tout=tf.uint8)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        display.clear_output()\n",
    "        \n",
    "        def ren(m):\n",
    "            self.images_renderer.render(model=self.model, dataset=test_ds, batch=epoch)\n",
    "            return 1\n",
    "        tf.numpy_function(ren, (1,), Tout=tf.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_fn = PeceptualLoss()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "@tf.function\n",
    "def mse_fn(x,y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32)\n",
    "    return tf.math.minimum(mse(x,y), tf.float32.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/denoiser\")\n",
    "# # model.load_weights(\"weights/denoiser_good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_large_memory_vars():\n",
    "    large_mem_files = list(filter(lambda x:(x[1]/1e6) > 1, [(name, sys.getsizeof(value)) for name, value in globals().items()]))\n",
    "\n",
    "    if len(large_mem_files) > 0:\n",
    "        print(f\"Found large files: {large_mem_files}. Delete?\")\n",
    "        i = input()\n",
    "        if \"y\" in i:\n",
    "            for (name, size) in large_mem_files:\n",
    "                del globals()[name]\n",
    "            print(f\"Deleted '{name}''\")\n",
    "check_for_large_memory_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, h):\n",
    "    if can_save(h):\n",
    "        print(\"Saving weights\")\n",
    "        model.save_weights(\"weights/denoiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_small_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "\n",
    "pl_logdir = f\"prepa_autencoder_logs/ae_xs_mse\"\n",
    "pl_tbutil = TensorboardUtil(pl_logdir)\n",
    "\n",
    "metrics=[\"mse\",\"accuracy\"]\n",
    "\n",
    "callbacks = [\n",
    "    RenderCallback(pl_tbutil),\n",
    "    pl_tbutil.get_callback(),\n",
    "#     tf.keras.callbacks.TensorBoard(log_dir = \"prepa_autencoder_logs/ae_xs_pl\",\n",
    "#                                                  histogram_freq = 1),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=31),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3)\n",
    "]\n",
    "\n",
    "def clipped_mse(x,y):\n",
    "    x = tf.cast(x, tf.float32)\n",
    "    y = tf.cast(y, tf.float32) \n",
    "    return mse(x,y)\n",
    "\n",
    "# optimizer = get_optimizer(5e-5)\n",
    "optimizer = tf.keras.optimizers.experimental.Nadam(learning_rate=5e-5)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=clipped_mse, metrics=[*metrics, LRMetric(optimizer)])\n",
    "\n",
    "h = model.fit(train_ds, epochs=epochs, validation_data=test_ds, callbacks=callbacks)\n",
    "save(model, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights/denoiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YUV MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "\n",
    "yuv_logdir = f\"prepa_autencoder_logs/ae_xs_yuv\"\n",
    "yuv_tbutil = TensorboardUtil(yuv_logdir)\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    RenderCallback(yuv_tbutil),\n",
    "    yuv_tbutil.get_callback(),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=31),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.7) #, min_delta=1\n",
    "]\n",
    "\n",
    "metrics=[\"mse\",\"accuracy\"]\n",
    "\n",
    "mae = tf.keras.losses.MeanAbsoluteError()\n",
    "def clipped_mse(x,y):\n",
    "    x = tf.cast(x , tf.float32)\n",
    "    y = tf.cast(y , tf.float32) \n",
    "    mse_val = mae(tf.image.rgb_to_yuv(x),tf.image.rgb_to_yuv(y))\n",
    "    return tf.cast(mse_val, tf.float32)\n",
    "\n",
    "# optimizer = get_optimizer(5e-5)\n",
    "optimizer = tf.keras.optimizers.experimental.Nadam(learning_rate=1e-5)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=clipped_mse, metrics=[*metrics, LRMetric(optimizer)], jit_compile=False)\n",
    "\n",
    "h = model.fit(train_ds, epochs=epochs, validation_data=test_ds, callbacks=callbacks)\n",
    "save(model, h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_small_decoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 100\n",
    "pl_logdir = f\"prepa_autencoder_logs/ae_xs_pl\"\n",
    "pl_tbutil = TensorboardUtil(pl_logdir)\n",
    "\n",
    "metrics=[\"mse\",\"accuracy\"]\n",
    "\n",
    "callbacks = [\n",
    "    RenderCallback(pl_tbutil),\n",
    "    pl_tbutil.get_callback(),\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=30),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(patience=5, cooldown=2, factor=0.7)\n",
    "]\n",
    "\n",
    "optimizer = get_optimizer(1e-4)\n",
    "# optimizer = tf.keras.optimizers.experimental.Nadam(learning_rate=1e-4)#, weight_decay=0.001)\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[*metrics, LRMetric(optimizer)])\n",
    "\n",
    "\n",
    "h = model.fit(train_ds, epochs=epochs,  validation_data=test_ds, callbacks=callbacks)\n",
    "save(model, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptual Loss (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs= 50\n",
    "# pl_logdir = f\"prepa_autencoder_logs/ae_xs_pl_sgd\"\n",
    "# pl_tbutil = TensorboardUtil(pl_logdir)\n",
    "\n",
    "# metrics=[\"mse\",\"accuracy\"]\n",
    "\n",
    "# callbacks = [\n",
    "#     RenderCallback(pl_tbutil),\n",
    "#     pl_tbutil.get_callback(),\n",
    "#     tf.keras.callbacks.EarlyStopping(patience=31),\n",
    "#     tf.keras.callbacks.ReduceLROnPlateau(patience=6)\n",
    "# ]\n",
    "\n",
    "# # optimizer = get_optimizer(5e-5)\n",
    "# optimizer = tf.keras.optimizers.experimental.SGD(learning_rate=1e-6, momentum=0.9, nesterov=True, clipnorm=1.0)\n",
    "\n",
    "\n",
    "# model.compile(optimizer=optimizer, loss=loss_fn, metrics=[*metrics, LRMetric(optimizer)])\n",
    "\n",
    "\n",
    "# h = model.fit(train_ds, epochs=epochs,  validation_data=test_ds, callbacks=callbacks)\n",
    "# save(model, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_small_decoder()\n",
    "# model.load_weights(\"weights/denoiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"weights/denoiser\")\n",
    "# model.save_weights(\"weights/denoiser_good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(\"weights/denoiser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepa dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def gen_img(img_path, img_type):\n",
    "    raw_png = tf.io.read_file(str(img_path), name=img_path)\n",
    "    return tf.image.decode_png(raw_png, channels=3, name=img_path)\n",
    "\n",
    "def gen_prepa():\n",
    "    prepa = [(str(p),'png') for p in pathlib.Path(\"./prepa\").glob('*.png')]\n",
    "    for img_path, img_type in random.sample(prepa, 10):\n",
    "        yield gen_img(img_path, img_type), img_path\n",
    "\n",
    "\n",
    "def test_prepa(autoencoder, examples=10): \n",
    "    \n",
    "    it = iter(gen_prepa())    \n",
    "    for i in range(examples):\n",
    "        \n",
    "        img, img_path = next(it)\n",
    "        \n",
    "        \n",
    "        images = [(\"Original\", img, img_path)]\n",
    "        \n",
    "        y_pred = autoencoder.predict(tf.expand_dims(img, axis=0), verbose=0)\n",
    "        y_pred = tf.cast(y_pred, tf.uint8)[0]\n",
    "        images.append((\"Model\", y_pred, img_path))\n",
    "        \n",
    "#         gaussian = tfa.image.gaussian_filter2d(tf.expand_dims(img, axis=0))\n",
    "#         images.append((\"gaussian\", gaussian[0]))\n",
    "        \n",
    "        count = len(images)\n",
    "        \n",
    "        fig = plt.figure(figsize=(10*count,20))\n",
    "        gs = fig.add_gridspec(count, hspace=0)\n",
    "        axs = gs.subplots()\n",
    "        for i, (image) in enumerate(images):\n",
    "            title, im, img_path = image\n",
    "#             print(img_path)\n",
    "            axs[i].set_title(title + \" \" + img_path, y=1.0, pad=-14)\n",
    "            axs[i].axis('off')\n",
    "            axs[i].imshow(im)\n",
    "        \n",
    "     \n",
    "        \n",
    "        \n",
    "\n",
    "test_prepa(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "initial_epoch, epochs = em.add_epochs(50)\n",
    "optimizer = get_optimizer(1e-6)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=[*metrics, LRMetric(optimizer)])\n",
    "\n",
    "\n",
    "h = model.fit(train_ds, epochs=epochs, initial_epoch=initial_epoch, validation_data=test_ds, callbacks=callbacks)\n",
    "save(model, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrt = 0.001\n",
    "epochs = 20\n",
    "\n",
    "def decayed_learning_rate(step):\n",
    "    decay_rate = 0.9\n",
    "    end_lr = 0.8\n",
    "    \n",
    "    return numpy.log(end_lr / 1e-3, step / epochs)\n",
    "\n",
    "# end_lr = initial_learning_rate * x ^ (step / decay_steps)\n",
    "# end_lr / initial_learning_rate = x ^ (step / decay_steps)\n",
    "\n",
    "\n",
    "def get_exploration_schedule(initial_lr, epochs):\n",
    "    return tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_lr,\n",
    "        decay_steps=epochs,\n",
    "        decay_rate=0.01,\n",
    "        staircase=False)\n",
    "\n",
    "lr_schedule = get_exploration_schedule(lrt, epochs)\n",
    "\n",
    "for i in range(30):\n",
    "    lrt = lr_schedule(i)\n",
    "    print(lrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "\n",
    "lr = 1e-4\n",
    "model_name = \"aenc/xs_v1.1\"\n",
    "dec_size = DecoderSize.md\n",
    "\n",
    "\n",
    "\n",
    "def get_discriminator(input_shape=(480,720,3)):\n",
    "    \n",
    "    def conv(filters, kernel_size=3, strides=1, dilation_rate=(1,1)):\n",
    "        return layers.Conv2D(filters,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 padding=\"same\",\n",
    "                 use_bias=True,\n",
    "                dilation_rate=dilation_rate,\n",
    "                 kernel_regularizer=tf.keras.regularizers.L2(1e-2))\n",
    "    \n",
    "    def convblock(filters, x, strides=2, dilation_rate=(1,1)):\n",
    "        if (strides > 1):\n",
    "            x = layers.MaxPool2D(strides)(x)\n",
    "        x = conv(filters, strides=1, dilation_rate=dilation_rate)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.LeakyReLU()(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    inputs1 = keras.Input(shape=input_shape, name=\"disc_input\")\n",
    "    input_scaled = tf.keras.layers.Rescaling(1./255)(inputs1)\n",
    "    \n",
    "    y = convblock(32, input_scaled, strides=1, dilation_rate=5)\n",
    "    x = convblock(32, input_scaled, strides=1)\n",
    "    \n",
    "    x = tf.add(x,y)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = convblock(64, x, strides=2)\n",
    "    y = convblock(64, x, strides=1, dilation_rate=5)\n",
    "    x = tf.add(x,y)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = convblock(128, x, strides=2)\n",
    "    x = convblock(512, x, strides=2)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    x = convblock(512, x, strides=2)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    x = convblock(512, x, strides=2)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    x = convblock(256, x, strides=1)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    x = convblock(128, x, strides=1)\n",
    "    x = layers.SpatialDropout2D(0.2)(x)\n",
    "    x = convblock(64, x, strides=1)\n",
    "#     x = convblock(32, x, strides=2)\n",
    "#     x = down(512, x, strides=2)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(512, activation=\"leaky_relu\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1024, activation=\"leaky_relu\")(x)\n",
    "#     x = layers.Dense(256, activation=\"leaky_relu\")(x)\n",
    "#     x = layers.Dense(32, activation=\"leaky_relu\", dtype=tf.float16)(x)\n",
    "    x = layers.Dense(1, activation=\"softmax\")(x)\n",
    "    return tf.keras.Model(inputs1, x, name=\"discriminator\")\n",
    "\n",
    "\n",
    "discriminator = get_discriminator((*dec_size.value,3))\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tbutil = TensorboardUtil(\"prepa_autencoder_logs/encoder_mse\")\n",
    "# dir(tbutil)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "loss_fn = PeceptualLoss()\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "batch_size = 2\n",
    "train_ds = div2k_ds(\"train\", dec_size.value).batch(batch_size)\n",
    "test_ds = div2k_ds(\"validation\", dec_size.value).batch(batch_size)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "def scheduler1(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5\n",
    "sch1 = tf.keras.callbacks.LearningRateScheduler(scheduler1)\n",
    "\n",
    "images_renderer = ImageRenderer(4).withTensorboard(TensorboardUtil(\"prepa_autencoder_logs/encoder_mse\"))\n",
    "def on_batch_end(epoch, logs):\n",
    "    if epoch % 200 == 0:\n",
    "        images_renderer.render(model=lambda x:encoder.predict(x, verbose=0), datasets=(train_ds, test_ds), batch=epoch)\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    display.clear_output()\n",
    "        \n",
    "render = tf.keras.callbacks.LambdaCallback(on_batch_end=on_batch_end, on_epoch_end=on_epoch_end)\n",
    "# reduce_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "#                 monitor=\"loss\",\n",
    "#                 factor=0.8,\n",
    "#                 patience=2,\n",
    "#                 cooldown=1,\n",
    "#                 min_lr=1e-8)\n",
    "\n",
    "encoder.compile(optimizer=optimizer, loss=mse, metrics=[\"accuracy\",\"mse\"])\n",
    "\n",
    "encoder.fit(train_ds, epochs=10, validation_data=test_ds, callbacks=[render, tbutil.get_callback(), sch1])\n",
    "\n",
    "def scheduler1(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return 1e-4\n",
    "    if epoch < 20:\n",
    "        return 1e-5\n",
    "    if epoch < 30:\n",
    "        return 1e-6\n",
    "    else:\n",
    "        return 1e-7\n",
    "sch1 = tf.keras.callbacks.LearningRateScheduler(scheduler1)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "encoder.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\",\"mse\"])\n",
    "\n",
    "encoder.fit(train_ds, epochs=40, validation_data=test_ds, callbacks=[render, tbutil.get_callback(), sch1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save_weights(\"weights/encoder_mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.load_weights(\"weights/encoder_mse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = get_discriminator((*dec_size.value,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_weights(\"weights/discriminator_mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"weights/denoiser\")\n",
    "encoder = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "batch_size = 2\n",
    "epochs = 30\n",
    "epoch_start_enc_train = 0\n",
    "\n",
    "\n",
    "\n",
    "is_enc_training = True\n",
    "\n",
    "def get_optimizer(lr=1e-4):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    optimizer = mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    return optimizer\n",
    "\n",
    "opt_enc =get_optimizer(1e-4)\n",
    "opt_disc =get_optimizer(1e-4)\n",
    "\n",
    "images_renderer = ImageRenderer(4).withTensorboard(TensorboardUtil(\"prepa_autencoder_logs/discriminator\"))\n",
    "print(f\"Output size: {dec_size.value}\")\n",
    "train_ds = div2k_ds(\"train\", dec_size.value).batch(batch_size)\n",
    "test_ds = div2k_ds(\"validation\", dec_size.value).batch(batch_size)\n",
    "\n",
    "train_writer = tf.summary.create_file_writer(\"prepa_autencoder_logs/discriminator/train\")\n",
    "val_writer = tf.summary.create_file_writer(\"prepa_autencoder_logs/discriminator/test\")\n",
    "\n",
    "int_spec = tf.TensorSpec(shape=[], dtype=tf.int32)\n",
    "image_spec1 = tf.TensorSpec(shape=[None,480, 720, 3], dtype=tf.float16)\n",
    "image_spec = tf.TensorSpec(shape=[None,*dec_size.value,3], dtype=tf.float16)\n",
    "int_spec = tf.TensorSpec(shape=[], dtype=tf.int32)\n",
    "bool_spec = tf.TensorSpec(shape=[], dtype=tf.bool)\n",
    "disc_output_spec = tf.TensorSpec(shape=[None,1], dtype=tf.float16, name=\"disc_output\")\n",
    "\n",
    "# ploss = LossNetwork()\n",
    "\n",
    "def resize(x):\n",
    "    return tf.image.resize(\n",
    "            x,\n",
    "            (dec_size.value[0], dec_size.value[1]),\n",
    "            preserve_aspect_ratio=False,\n",
    "            antialias=False,\n",
    "            method=\"nearest\",\n",
    "            name=None)\n",
    "\n",
    "# @tf.function(input_signature=(disc_output_spec,disc_output_spec))\n",
    "def disc_loss_fn(y_pred,y_true):\n",
    "    l1 = loss_fn(tf.ones_like(y_true),y_true)\n",
    "    l2 = loss_fn(tf.zeros_like(y_pred),y_pred)\n",
    "    return tf.cast(l1+l2, tf.float32)\n",
    "\n",
    "# @tf.function(input_signature=(disc_output_spec,disc_output_spec))\n",
    "def enc_loss_fn(y_pred):\n",
    "    l1 = loss_fn(tf.ones_like(y_pred), y_pred)\n",
    "    return tf.cast(l1, tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_val():\n",
    "    enc_loss = 0\n",
    "    disc_loss = 0\n",
    "    steps = 0\n",
    "    for step, (x, y) in enumerate(test_ds):\n",
    "        steps+=1\n",
    "\n",
    "        x = encoder(x, training=False)\n",
    "    \n",
    "#         z = resize(y)\n",
    "        y_pred = discriminator(x, training=False)\n",
    "        y_true = discriminator(y, training=False)\n",
    "#         noise = discriminator(z, training=False)\n",
    "        \n",
    "        dloss = disc_loss_fn(y_pred,y_true)\n",
    "        eloss = enc_loss_fn(y_pred)\n",
    "#         eloss = mse(x, y)\n",
    "\n",
    "        enc_loss += eloss\n",
    "        disc_loss += dloss\n",
    "    return enc_loss/steps, disc_loss/steps\n",
    "        \n",
    "def update_gradients(model, tape, optimizer, scaled_loss):\n",
    "    scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n",
    "    gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "@tf.function(input_signature=(int_spec, int_spec ,image_spec1, image_spec, bool_spec))\n",
    "def train_step(epoch, step, x, y, is_enc_training):\n",
    "    # Open a GradientTape to record the operations run\n",
    "    # during the forward pass, which enables auto-differentiation.\n",
    "    tape_enc = tf.GradientTape()\n",
    "    tape_disc = tf.GradientTape()\n",
    "    with tape_enc:\n",
    "#         is_enc_training = epoch >= epoch_start_enc_train\n",
    "        x = encoder(x, training=is_enc_training)\n",
    "        \n",
    "   \n",
    "    with tape_disc, tape_enc:\n",
    "#         z = resize(y)\n",
    "        y_pred = discriminator(x, training=True)\n",
    "        y_true = discriminator(y, training=True)\n",
    "#         noise = discriminator(z, training=True)\n",
    "#         print(f\"Sizes: y_pred={y_pred.shape} y_true={y_true.shape}\")\n",
    "        \n",
    "        dloss = disc_loss_fn(y_pred,y_true)\n",
    "        dscaled_loss = opt_disc.get_scaled_loss(dloss)\n",
    "\n",
    "        eloss = tf.cast(0.0, tf.float32)\n",
    "        escaled_loss = tf.cast(0.0, tf.float32)\n",
    "        mse_error = tf.cast(0.0, tf.float32)\n",
    "        if is_enc_training:\n",
    "            mse_error =  mse(tf.cast(x, tf.float32),tf.cast(y, tf.float32))\n",
    "            eloss = enc_loss_fn(y_pred)\n",
    "    #             eloss = mse(x, y)\n",
    "            escaled_loss = opt_enc.get_scaled_loss(eloss * mse_error)\n",
    "\n",
    "        \n",
    "        \n",
    "#         loss_value = calculate_loss(x_batch_train,y_batch_train, training=True)\n",
    "\n",
    "    if is_enc_training:\n",
    "        update_gradients(encoder, tape_enc, opt_enc, escaled_loss)\n",
    "    update_gradients(discriminator, tape_disc, opt_disc, dscaled_loss)\n",
    "    return eloss, dloss, mse_error\n",
    "\n",
    "\n",
    "images_renderer.render(model=lambda x:encoder.predict(x, verbose=0), datasets=(train_ds, test_ds), batch=0)\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "    \n",
    "    epoch_enc_loss = 0\n",
    "    epoch_disc_loss = 0\n",
    "    epoch_mse = 0\n",
    "    steps = 0\n",
    "    # Iterate over the batches of the dataset.\n",
    "    progress = iter(trange(len(train_ds)))\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "        steps += 1\n",
    "        encoder_loss, disc_loss, mse_error = train_step(epoch, step, x_batch_train, y_batch_train, is_enc_training)\n",
    "        epoch_enc_loss += encoder_loss\n",
    "        epoch_disc_loss += disc_loss\n",
    "        epoch_mse += mse_error\n",
    "        next(progress)\n",
    "        # Log every 200 batches.\n",
    "        \n",
    "        if step % 20 == 0:\n",
    "            print(f\"encoder_loss={encoder_loss}, discriminator_loss={disc_loss} mse={mse_error}\")\n",
    "#             print(\"Seen so far: %s samples\" % ((step + 1) * batch_size))\n",
    "    val_enc_loss, val_disc_loss = calculate_val()\n",
    "    display.clear_output()\n",
    "    with train_writer.as_default():\n",
    "        tf.summary.scalar(\"enc_loss\", epoch_enc_loss/steps, step=epoch)\n",
    "        tf.summary.scalar(\"disc_loss\", epoch_disc_loss/steps, step=epoch)\n",
    "        tf.summary.scalar(\"mse\", epoch_mse/steps, step=epoch)\n",
    "    with val_writer.as_default():\n",
    "        tf.summary.scalar(\"enc_loss\", val_enc_loss, step=epoch)\n",
    "        tf.summary.scalar(\"disc_loss\", val_disc_loss, step=epoch)\n",
    "    images_renderer.render(model=lambda x:encoder.predict(x, verbose=0), datasets=(train_ds, test_ds), batch=epoch)\n",
    "    \n",
    "    print(f\"Validation enc_loss = {val_enc_loss} disc_loss = {val_disc_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save_weights(\"weights/discriminator_mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder.save_weights(\"weights/encoder_mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prepa(model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = get_optimizer(1e-5)\n",
    "def combined_loss(x,y):\n",
    "    discriminator(x, training=False)\n",
    "    return (loss_fn(x,y)) * mse_fn(x,y)\n",
    "\n",
    "model.compile(optimizer=get_optimizer(lr=1e-5), loss=combined_loss, metrics=[\"accuracy\",\"mse\"])\n",
    "\n",
    "\n",
    "model.fit(train_ds, epochs=50, validation_data=test_ds, callbacks=[render, tbutil.get_callback()])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 with Fil",
   "language": "python",
   "name": "filprofile"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "4327198819dafd55a2243f22aba11bf2a7d9f0c32aced8ba7d18a900e49d0553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
