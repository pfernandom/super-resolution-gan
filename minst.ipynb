{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten,\\\n",
    "                                    Reshape, LeakyReLU as LR,\\\n",
    "                                    Activation, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform(\n",
    "    (2,12,12,1),\n",
    "    minval=0,\n",
    "    maxval=1,\n",
    "    dtype=tf.dtypes.float32,\n",
    ")\n",
    "\n",
    "h = 12\n",
    "w = 12\n",
    "c = 1\n",
    "filter_len = 3\n",
    "\n",
    "im = tf.reshape(tf.constant(range(2*12*12*1)), (2,12,12,1))\n",
    "\n",
    "mask = tf.reshape(tf.range(h*w*c), (h,w,c))\n",
    "\n",
    "\n",
    "i = random.randint(0,h-filter_len)\n",
    "j = random.randint(0,w-filter_len)\n",
    "\n",
    "a = tf.math.logical_and(mask / h <= j+filter_len, mask % h <= i+filter_len)\n",
    "b = tf.math.logical_and(mask / w > j, mask % w > i)\n",
    "d = tf.math.logical_and(a,b)\n",
    "\n",
    "for i in range(3):\n",
    "    i = random.randint(0,h-filter_len)\n",
    "    j = random.randint(0,w-filter_len)\n",
    "\n",
    "    a = tf.math.logical_and(mask / h <= j+filter_len, mask % h <= i+filter_len)\n",
    "    b = tf.math.logical_and(mask / w > j, mask % w > i)\n",
    "    c = tf.math.logical_and(a,b)\n",
    "    d = tf.math.logical_or(d,c)\n",
    "\n",
    "\n",
    "tf.where(d, tf.zeros_like(im), im)[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = tf.reshape(tf.constant(range(2*12*12*1)), (2,12,12,1))\n",
    "print(im[:,:,:,0])\n",
    "patches = tf.image.extract_patches(images=im,\n",
    "                           sizes=[1, 4, 4, 1],\n",
    "                           strides=[1, 1, 1, 1],\n",
    "                           rates=[1, 1, 1, 1],\n",
    "                           padding='VALID')\n",
    "\n",
    "zeros = tf.zeros(patches.shape[-1], dtype=tf.int32)\n",
    "\n",
    "patches * zeros\n",
    "\n",
    "\n",
    "# patches = tf.extract_volume_patches(im, [2,2,2,2,1], [2,2,2,2,2], \"SAME\")\n",
    "# def build_negative_filter(img, filter=2, masks=6):\n",
    "\n",
    "#     patches = tf.extract_volume_patches(img, [2,2,2,2])\n",
    "#     # print((n,h,w,c))\n",
    "#     # i = [random.randint(0, h-filter) for _ in range(masks)]\n",
    "#     # j = [random.randint(0, w-filter) for _ in range(masks)]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     # def to_zero(y):\n",
    "#     #     x1, x2 = y\n",
    "\n",
    "#     #     row = int(x1/h)\n",
    "#     #     col = x1 % h\n",
    "#     #     # print(row, col)\n",
    "\n",
    "#     #     for k in range(len(i)):\n",
    "#     #         if row >= i[k] and row <= i[k] + filter and col >= j[k] and col <= j[k] + filter:\n",
    "#     #             return 0\n",
    "\n",
    "#     #     return 1\n",
    "\n",
    "#     # return tf.reshape(tf.constant(list(map(to_zero, enumerate(range(n*h*w*c))))), (n,h,w,c))\n",
    "\n",
    "# ones = tf.ones_like(im)\n",
    "# twos = ones * 2\n",
    "# masked = tf.math.multiply(im, build_negative_filter())\n",
    "# r = masked + twos - ones\n",
    "# r[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoiseUtil:\n",
    "    @staticmethod\n",
    "    def filter_pixel(downsize_image_ratios = [1/16]):\n",
    "        def fn(img):\n",
    "            downsize_image_ratio = random.choice(downsize_image_ratios)\n",
    "            resized_size_h = img.shape[1]\n",
    "            resized_size_w = img.shape[2]\n",
    "            \n",
    "            # noisy = normalize(img) + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=(img.shape))\n",
    "            # noisy = layers.MaxPooling2D(pool_size = (8, 8), padding='same')(tf.expand_dims(img, axis=0))\n",
    "            down = tf.image.resize(\n",
    "                img,\n",
    "                [int(resized_size_h * downsize_image_ratio), int(resized_size_w * downsize_image_ratio)],\n",
    "                preserve_aspect_ratio=True,\n",
    "                antialias=False,\n",
    "                name=None)\n",
    "\n",
    "            up = tf.image.resize(\n",
    "                down,\n",
    "                [resized_size_h, resized_size_w],\n",
    "                preserve_aspect_ratio=True,\n",
    "                antialias=False,\n",
    "                name=None)\n",
    "\n",
    "            up = tf.cast((up), img.dtype)\n",
    "            return up\n",
    "        return fn\n",
    "\n",
    "    @staticmethod\n",
    "    def random_pixel_noise(img2, filter_size, patch_size, filter_fn, shape):\n",
    "        \n",
    "        if (len(tf.shape(img2)) < 4):\n",
    "            img2 = tf.convert_to_tensor(img2[:,:,:])\n",
    "            img2 = tf.expand_dims(img2, axis=0)\n",
    "        else:\n",
    "            img2 = tf.convert_to_tensor(img2[:,:,:,:])\n",
    "  \n",
    "        img2_filtered = filter_fn(img2)\n",
    "        print(f\"shape={shape}\")\n",
    "        shp = shape\n",
    "        n = shp[0]\n",
    "        h = shp[1]\n",
    "        w = shp[2]\n",
    "        c = shp[3]\n",
    "\n",
    "        print(h)\n",
    "\n",
    "        # mask = build_negative_filter(n, h, w, c, filter=patch_size, masks=filters)\n",
    "        # x = tf.random.uniform(\n",
    "        #     tf.shape(img2),\n",
    "        #     minval=0,\n",
    "        #     maxval=2,\n",
    "        #     dtype=tf.int32,\n",
    "        # )\n",
    "        mask = tf.reshape(tf.range(h*w*c), (h,w,c))\n",
    "\n",
    "        i = random.randint(0,h-patch_size)\n",
    "        j = random.randint(0,w-patch_size)\n",
    "\n",
    "        a = tf.math.logical_and(mask / h <= j+patch_size, mask % h <= i+patch_size)\n",
    "        b = tf.math.logical_and(mask / w > j, mask % w > i)\n",
    "        d = tf.math.logical_and(a,b)\n",
    "\n",
    "        for i in range(filter_size):\n",
    "            i = random.randint(0,h-patch_size)\n",
    "            j = random.randint(0,w-patch_size)\n",
    "\n",
    "            a = tf.math.logical_and(mask / h <= j+patch_size, mask % h <= i+patch_size)\n",
    "            b = tf.math.logical_and(mask / w > j, mask % w > i)\n",
    "            c = tf.math.logical_and(a,b)\n",
    "            d = tf.math.logical_or(d,c)\n",
    "\n",
    "\n",
    "        # im = tf.reshape(tf.constant(range(2*12*12*1)), (2,12,12,1))\n",
    "        img3 = tf.where(d, tf.zeros_like(img2), img2)\n",
    "        # img3 = tf.where(x == 0, img2, tf.zeros_like(img2))\n",
    "\n",
    "        # img3 = tf.math.multiply(img2, mask)\n",
    "        img4 = img2_filtered * 2\n",
    "        img2 = img3 + img4 - img2_filtered\n",
    "\n",
    "\n",
    "        # starts = random.sample(range(10000000), filters)\n",
    "        \n",
    "        # for i in range(filters):\n",
    "        #     assert patch_size > 0\n",
    "        #     assert patch_size % 2 == 0\n",
    "        #     assert patch_size < img2.shape[1]-4\n",
    "        #     assert patch_size < img2.shape[2]-4\n",
    "\n",
    "        #     random.seed(time.time())\n",
    "\n",
    "        #     s1 = random.randint(0,img2.shape[1]-patch_size-4)\n",
    "        #     s2 = random.randint(0,img2.shape[2]-patch_size-4)\n",
    "            \n",
    "        #     img3 = tfa.image.cutout(img2, mask_size=[patch_size,patch_size], offset= [s1,s2])\n",
    "        #     img4 = tfa.image.cutout(img2_filtered, mask_size=[patch_size,patch_size], offset= [s1,s2])\n",
    "\n",
    "        #     img2 = img3 + img2_filtered - img4\n",
    "        \n",
    "        return img2\n",
    "\n",
    "    @staticmethod\n",
    "    def pixel_noise(img, filters, filter_size):\n",
    "        return NoiseUtil.random_pixel_noise(img, filters, filter_size, filter_fn=NoiseUtil.filter_pixel(downsize_image_ratios = [1/4]), shape=img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgUtils():\n",
    "    @staticmethod\n",
    "    def normalize(image, as_probs=False):\n",
    "        a = (tf.cast(image, tf.float32) - 127.5) if not as_probs else tf.cast(image, tf.float32)\n",
    "        return a / 127.5\n",
    "\n",
    "    @staticmethod\n",
    "    def denormalize(image, cast=False):\n",
    "        # return image\n",
    "        img = tf.cast(image, tf.float32) * 127.5 + 127.5\n",
    "        img = tf.clip_by_value(img, 0.0, 255.0)\n",
    "        if cast:\n",
    "            return tf.cast(img, cast)\n",
    "        return img\n",
    "\n",
    "im = np.random.normal(loc=0.0, scale=1.0, size=(1,28,28,1))\n",
    "im = tf.cast(im, tf.float32) * 127.5 + 127.5\n",
    "im = tf.clip_by_value(im, 0.0, 255.0)\n",
    "\n",
    "im_processed = ImgUtils.denormalize(ImgUtils.normalize(im), cast=im.dtype)\n",
    "\n",
    "assert np.any(im == im_processed)\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "\n",
    "    @staticmethod\n",
    "    def one_from_generator(fn, shape):\n",
    "        output_signature=tf.TensorSpec(shape=shape, dtype=tf.float32)\n",
    "        return tf.data.Dataset.from_generator(fn, output_signature=output_signature)\n",
    "\n",
    "    @staticmethod\n",
    "    def two_from_generator(fn, shape):\n",
    "        output_signature=tf.TensorSpec(shape=shape, dtype=tf.float32)\n",
    "        return tf.data.Dataset.from_generator(fn, output_signature=output_signature).map(lambda x: (x,x))\n",
    "\n",
    "class DataManager():\n",
    "    def __init__(self, train_ds, test_ds = None):\n",
    "        self.train_ds = train_ds\n",
    "        self.test_ds = test_ds\n",
    "\n",
    "    def set_test_samples(self, test_samples=25):\n",
    "        self.test_samples = test_samples\n",
    "\n",
    "    def set_transform(self, fn):\n",
    "        self.transform = fn\n",
    "\n",
    "    @staticmethod\n",
    "    def create_label_with_input_transform(train_generator, test_generator, input_shape, transform_fn=lambda x:x, test_samples=25):\n",
    "        def normalize_both(x,y):\n",
    "            return ImgUtils.normalize(x), ImgUtils.normalize(y)\n",
    "        \n",
    "        train_ds = DataLoader.two_from_generator(train_generator, input_shape).map(normalize_both).skip(test_samples)\n",
    "        test_ds = None\n",
    "        if test_generator:\n",
    "            test_ds = DataLoader.two_from_generator(test_generator, input_shape).map(normalize_both).take(test_samples)\n",
    "\n",
    "        dm = DataManager(train_ds, test_ds)\n",
    "        dm.set_test_samples(test_samples)\n",
    "        dm.set_transform(transform_fn)\n",
    "\n",
    "        return dm\n",
    "\n",
    "    def get_test_data(self, batch_size):\n",
    "        return self.test_ds.take(batch_size).batch(batch_size).map(self.transform)\n",
    "\n",
    "    def get_training_data(self, batch_size):\n",
    "        return self.train_ds.take(batch_size).batch(batch_size).map(self.transform)\n",
    "\n",
    "    def print_validation(self, model=lambda x:x, batch_size=5, save=False, path=\"./\"):\n",
    "        random.seed(10)\n",
    "        rows = batch_size\n",
    "        cols = 2\n",
    "\n",
    "        results = [(model(x), y) for x,y in self.get_test_data(batch_size)]\n",
    "        plt.figure(figsize=(rows * 2, cols * 2))\n",
    "        for x,y in results:\n",
    "            assert x.shape == y.shape\n",
    "            for i in range(x.shape[0]):\n",
    "                im = x[i,:,:,:]\n",
    "                plt.subplot(cols, rows, i+1)\n",
    "                plt.imshow(ImgUtils.denormalize(im, cast=tf.uint8))\n",
    "                plt.axis('off')\n",
    "           \n",
    "\n",
    "            for i in range(y.shape[0]):\n",
    "                im = y[i,:,:,:]\n",
    "                plt.subplot(cols, rows, i+x.shape[0]+1)\n",
    "                plt.imshow(ImgUtils.denormalize(im, cast=tf.uint8))\n",
    "                plt.axis('off')\n",
    "        plt.subplots_adjust(wspace = 0, hspace = 0.5)\n",
    "        if save:\n",
    "            plt.savefig(path)\n",
    "        plt.show()\n",
    "           \n",
    "def train_get():\n",
    "   for x, y in zip(x_train, y_train):\n",
    "        x = tf.expand_dims(x, axis=2)\n",
    "        yield x\n",
    "\n",
    "def test_get():\n",
    "   for x in x_test:\n",
    "        x = tf.expand_dims(x, axis=2)\n",
    "        yield x\n",
    "\n",
    "\n",
    "def add_noise(x,y):\n",
    "    n = NoiseUtil.pixel_noise(x, 30, 2)\n",
    "    return n,y\n",
    "dm = DataManager.create_label_with_input_transform(train_get, test_get, (28,28,1), add_noise)\n",
    "dm.print_validation() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "def get_conv(filters):\n",
    "    conv = tf.keras.Sequential()\n",
    "    conv.add(tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "    ))\n",
    "    conv.add(layers.LeakyReLU())\n",
    "    return conv\n",
    "    \n",
    "def get_encoder():\n",
    "  encoder = tf.keras.Sequential(name=\"encoder\")\n",
    "  # encoder.add(layers.GaussianDropout(0.2))\n",
    "  encoder.add(get_conv(16))\n",
    "  encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "  encoder.add(get_conv(64))\n",
    "  encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "  encoder.add(get_conv(128))\n",
    "  return encoder\n",
    "\n",
    "def get_decoder():\n",
    "  decoder = tf.keras.Sequential(name=\"decoder\")\n",
    "  decoder.add(layers.UpSampling2D((2, 2), interpolation='bilinear'))\n",
    "  decoder.add(get_conv(64))\n",
    "  decoder.add(layers.UpSampling2D((2, 2), interpolation='bilinear'))\n",
    "  decoder.add(get_conv(16))\n",
    "  decoder.add(layers.Conv2D(1, 3, padding='same', activation='tanh'))\n",
    "  return decoder\n",
    "\n",
    "class AutoEncoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.encoder = get_encoder()\n",
    "    self.decoder = get_decoder()\n",
    "    \n",
    "    self.encoder.build(input_shape=input_shape)\n",
    "    \n",
    "    sh = self.encoder.output_shape\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    self.seq1 = layers.Dense(1024)\n",
    "    print(sh)\n",
    "    self.reshape = layers.Reshape([*sh[1:]])\n",
    "    \n",
    "    self.last = layers.Conv2D(1, 3, padding='same', activation='tanh')\n",
    "    self.inputs_dropout = layers.Dropout(0.2)\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.inputs_dropout(inputs)\n",
    "    x = self.encoder(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.reshape(x)\n",
    "    x = self.decoder(x)\n",
    "    \n",
    "    # x_inputs = self.conv_input(inputs)\n",
    "    \n",
    "    # x = self.add([x * self.gamma, x_inputs])\n",
    "    # x = self.attention(x)\n",
    "    x = self.last(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_images = []\n",
    "\n",
    "# def train_get():\n",
    "#    for x, y in zip(x_train, y_train):\n",
    "#         x = tf.expand_dims(x, axis=2)\n",
    "#         generated_images.append(y)\n",
    "#         yield x\n",
    "\n",
    "# def test_get():\n",
    "#    for x in x_test:\n",
    "#         x = tf.expand_dims(x, axis=2)\n",
    "#         yield x\n",
    "\n",
    "# output_signature=tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "# BATCH_SIZE = 25\n",
    "\n",
    "# def add_noise(x, filters=20, filter_size=4):\n",
    "#    x= random_pixel_noise(x, filters, filter_size, filter_fn=filter_pixel(downsize_image_ratios = [1/4, 1/8]))\n",
    "#    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "# def expand(x):\n",
    "#    return tf.expand_dims(x, axis=-1)\n",
    "\n",
    "# ds_x_train = tf.data.Dataset.from_generator(train_get, output_signature=output_signature).map(lambda x: (add_noise(x),expand(x))).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ds_x_test = tf.data.Dataset.from_generator(test_get, output_signature=output_signature).map(lambda x: (add_noise(x),expand(x))).take(BATCH_SIZE).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(next(iter(ds_x_train))[0].shape)\n",
    "# print_validation(lambda x: add_noise(x, filters=20, filter_size=4))\n",
    "# print_validation(lambda x: add_noise(x, filters=20, filter_size=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "  # def validation_step(self, images):\n",
    "  #   pass\n",
    "\n",
    "  # def train_step(self, images):\n",
    "  #   with tf.GradientTape() as auto_tape:\n",
    "  #     generated = self.autoencoder(images)\n",
    "  #     loss = cross_entropy(images, generated)\n",
    "  #   gradients = auto_tape.gradient(loss, self.autoencoder.trainable_variables)\n",
    "  #   opt.apply_gradients(zip(gradients, self.autoencoder.trainable_variables))\n",
    "\n",
    "  #   return {\"loss\": loss}\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f'logs/minst/{current_time}'\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = train_log_dir,\n",
    "  write_graph=True,\n",
    "  histogram_freq = 1,\n",
    "  update_freq=\"batch\"\n",
    "  )\n",
    "\n",
    "EPOCHS = 60\n",
    "model = AutoEncoder()\n",
    "\n",
    "class SkMetrics(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.validation_loss = []   \n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "      originals = []\n",
    "      predicted = []\n",
    "      def expand_and_predict(x):\n",
    "        result = model(x, training=False)\n",
    "        return result\n",
    "      if batch % 99 == 0 and batch > 0:\n",
    "        display.clear_output()\n",
    "        dm.print_validation(expand_and_predict)\n",
    "        \n",
    "class CustomMSE(keras.losses.Loss):\n",
    "    def __init__(self, name=\"custom_mse\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        mse = tf.math.reduce_mean(tf.square(y_true - y_pred))\n",
    "        return mse\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "# model.compile(run_eagerly=True)\n",
    "model.compile(loss=keras.losses.MeanSquaredError(), optimizer=opt, metrics=[\"mse\"])\n",
    "history = model.fit(dm.get_training_data(BATCH_SIZE).repeat(), epochs=100, steps_per_epoch=300, validation_data=dm.get_test_data(BATCH_SIZE), callbacks=[SkMetrics()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder2(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.encoder = get_encoder()\n",
    "    self.decoder = get_decoder()\n",
    "    \n",
    "    self.encoder.build(input_shape=input_shape)\n",
    "    \n",
    "    sh = self.encoder.output_shape\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    self.seq1 = layers.Dense(1024)\n",
    "    print(sh)\n",
    "    self.reshape = layers.Reshape([*sh[1:]])\n",
    "    \n",
    "    self.last = layers.Conv2D(1, 3, padding='same', activation='sigmoid')\n",
    "    self.inputs_dropout = layers.Dropout(0.2)\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.inputs_dropout(inputs)\n",
    "    x = self.encoder(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.reshape(x)\n",
    "    x = self.decoder(x)\n",
    "    \n",
    "    # x_inputs = self.conv_input(inputs)\n",
    "    \n",
    "    # x = self.add([x * self.gamma, x_inputs])\n",
    "    # x = self.attention(x)\n",
    "    x = self.last(x)\n",
    "    return x\n",
    "\n",
    "autoencoder2 = AutoEncoder2()\n",
    "autoencoder2.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "history = model.fit(dm.get_training_data().repeat(), epochs=55, steps_per_epoch=500, validation_data=dm.get_test_data(), callbacks=[SkMetrics(), tboard_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4327198819dafd55a2243f22aba11bf2a7d9f0c32aced8ba7d18a900e49d0553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
