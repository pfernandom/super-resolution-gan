{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/local/cuda-11.8/\n",
    "!export CUDA_DIR=\"/usr/local/cuda-11.8/\"\n",
    "!export TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(f\"gpus={gpus}\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten,\\\n",
    "                                    Reshape, LeakyReLU as LR,\\\n",
    "                                    Activation, Dropout\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display # If using IPython, Colab or Jupyter\n",
    "import numpy as np\n",
    "import tensorflow_addons as tfa\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://drive.google.com/uc?export=download&id=0B7EVK8r0v71pZjFTYXZWM3FlRnM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_img = tfds.load(\"coil100\", split=['train'])\n",
    "\n",
    "IMG_H = 128\n",
    "IMG_W = 128\n",
    "IMG_CHANNELS = 3\n",
    "\n",
    "EXPERIMENT_VERSION = \"v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "def gen():\n",
    "    for x in ds_img:\n",
    "        for y in x:\n",
    "            yield y[\"image\"]\n",
    "        \n",
    "output_signature=tf.TensorSpec(shape=(IMG_H, IMG_W, IMG_CHANNELS), dtype=tf.uint8)\n",
    "dataset = tf.data.Dataset.from_generator(gen, output_signature=output_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    a = (tf.cast(image, tf.float32) - 127.5) \n",
    "    return a / 127.5\n",
    "\n",
    "def denormalize(image):\n",
    "    # return image\n",
    "    return tf.clip_by_value(tf.cast(tf.cast(image, tf.float32) * 127.5 + 127.5, np.float32), 0.0, 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# x_train = x_train/255.0\n",
    "# x_test = x_test/255.0\n",
    "\n",
    "TEST_SAMPLES = 25\n",
    "\n",
    "x_test = dataset.take(TEST_SAMPLES)\n",
    "x_train = dataset.skip(TEST_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_samples = np.array([y for y in list(x_test.take(4))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_validation(fn=lambda x:x, save=False, path=\"./\"):\n",
    "    random.seed(10)\n",
    "    rows = 4\n",
    "    cols = 2\n",
    "\n",
    "    rand = zip(x_test_samples, fn(x_test_samples))\n",
    "    n = np.vstack((x_test_samples,fn(x_test_samples)))\n",
    "    print((cols, rows))\n",
    "\n",
    "    plt.figure(figsize=(rows * 2, cols * 2))\n",
    "    for i in range(n.shape[0]):\n",
    "        x = n[i,:,:,:]\n",
    "        plt.subplot(cols, rows, i+1)\n",
    "        plt.imshow(tf.cast(x, np.uint8))\n",
    "        plt.axis('off')\n",
    "    plt.subplots_adjust(wspace = 0, hspace = 0.5)\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "    plt.show()\n",
    "    \n",
    "print_validation(lambda x: denormalize(normalize(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "test_results_file = \"./testresults.txt\"\n",
    "\n",
    "class Experiment:\n",
    "    def __init__(self):\n",
    "        self.encoder = ()\n",
    "        self.decoder = ()\n",
    "        self.use_denorm = False\n",
    "        self.all_attention = False\n",
    "        self.use_batch_norm = False\n",
    "        \n",
    "    def toJSON(self):\n",
    "        return json.dumps(self, default=lambda o: o.__dict__, \n",
    "            sort_keys=True, indent=4)\n",
    "    \n",
    "    def set_auto_encoder_filters(self, encoder, decoder):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        return self\n",
    "        \n",
    "    def with_denorm(self, use_denorm):\n",
    "        self.use_denorm = use_denorm\n",
    "        return self\n",
    "        \n",
    "    def with_attention(self, all_a=True):\n",
    "        self.all_attention = all_a\n",
    "        return self\n",
    "        \n",
    "    def with_batch_norm(self, use_batch=False):\n",
    "        self.use_batch_norm = use_batch\n",
    "        return self\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"k\".join(map(lambda x:str(x), self.encoder)) + \"-\" + \"k\".join(map(lambda x:str(x), self.decoder)) \\\n",
    "            + (\"_idenorm\" if self.use_denorm else \"_inorm\") + \\\n",
    "            (\"_attn\" if self.all_attention else \"\") + \\\n",
    "            (\"_batch_norm\" if self.use_batch_norm else \"_spectral\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self.toJSON())\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.toJSON())\n",
    "    \n",
    "    def create_model(self):\n",
    "        enc_conf = self.encoder\n",
    "        dec_conf = self.decoder\n",
    "\n",
    "        use_attention = self.all_attention\n",
    "        use_batch_norm = self.use_batch_norm\n",
    "\n",
    "        return AutoEncoder(enc_conf, dec_conf, use_attention, use_batch_norm)\n",
    "    \n",
    "    def save(self):\n",
    "        f = open(test_results_file, \"a\")\n",
    "        f.write(self.get_name() + \"\\n\")\n",
    "        f.close()\n",
    "  \n",
    "\n",
    "all_filters = [\n",
    "    ([32,64,128,256,512], [256,128,64,32]),\n",
    "    ([16,32,64,128,512], [256,64,32,16]), # best\n",
    "    ([4,8,32,128,512], [128,32,8,4])\n",
    "]\n",
    "experiments = []\n",
    "if os.path.exists(test_results_file):\n",
    "    os.remove(test_results_file)\n",
    "for encf, decf in all_filters:\n",
    "    for use_den in [True, False]: # both equally good\n",
    "        for with_att in [True, False]: # true best\n",
    "            for with_batch_norm in [True, False]: # false best\n",
    "                experiments.append(Experiment()\n",
    "                    .set_auto_encoder_filters(encf, decf)\n",
    "                    .with_denorm(use_den)\n",
    "                    .with_attention(with_att)\n",
    "                    .with_batch_norm(with_batch_norm))\n",
    "\n",
    "list(map(lambda x: x.get_name(), experiments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, filters, use_batch_norm=False, activation=False):\n",
    "    super(AttentionLayer, self).__init__()\n",
    "    self.filters = filters\n",
    "    self.activation = activation\n",
    "    self.use_batch_norm = use_batch_norm\n",
    "    \n",
    "  def get_conv_t(self, filters, strides, use_batch_norm=False):\n",
    "    conv = tf.keras.Sequential()\n",
    "\n",
    "    if use_batch_norm:\n",
    "        conv.add(layers.Conv2DTranspose(filters, 3, strides=strides, padding=\"same\", use_bias=False))\n",
    "        conv.add(layers.BatchNormalization(momentum=0.3))\n",
    "    else:\n",
    "        conv.add(tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2DTranspose(filters, 3, strides=strides, padding=\"same\", use_bias=False)\n",
    "        ))\n",
    "        \n",
    "    conv.add(layers.LeakyReLU())\n",
    "    return conv\n",
    "    \n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.in_shape = input_shape\n",
    "    b, hi, wi, c = input_shape\n",
    "    self.downscale1 = layers.Conv2D(tf.cast(c*4, tf.int32), 4, strides = 4)\n",
    "    self.downscale2 = layers.Conv2D(tf.cast(c*2, tf.int32), 2, strides = 2)\n",
    "    self.fc = self._conv(self.filters)\n",
    "    self.gc = self._conv(self.filters)\n",
    "    self.hc = self._conv(self.filters)\n",
    "    self.xc = self._conv(self.filters)\n",
    "#     self.upscale1 = layers.Conv2DTranspose(self.filters, 3, strides=4, padding=\"same\", use_bias=False)\n",
    "#     self.upscale2 = layers.Conv2DTranspose(self.filters, 3, strides=2, padding=\"same\", use_bias=False)\n",
    "    self.upscale1 = self.get_conv_t(self.filters, 4, self.use_batch_norm)\n",
    "    self.upscale2 = self.get_conv_t(self.filters, 2, self.use_batch_norm)\n",
    "\n",
    "    self.gamma = tf.Variable([1.], name=\"gamma\")\n",
    "    self.act = layers.LeakyReLU()\n",
    "    if self.use_batch_norm:\n",
    "        self.norm = layers.BatchNormalization(momentum=0.3)\n",
    "#     self.downsized_dims = tf.cast(hi/6, tf.int32), tf.cast(wi/6, tf.int32), c*6\n",
    "    \n",
    "  def _conv(self, filters, kernel=1, strides=1):\n",
    "    return layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False)\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    ch = self.filters\n",
    "    xdsh = self.in_shape\n",
    "    b = xdsh[0]\n",
    "    hi = xdsh[1]\n",
    "    wi = xdsh[2]\n",
    "    c = xdsh[3]\n",
    "        \n",
    "    xd = inputs\n",
    "    xd = self.downscale1(xd)\n",
    "    xd = self.downscale2(xd)\n",
    "\n",
    "    xdsh = tf.shape(xd)\n",
    "    b = xdsh[0]\n",
    "    hi = xdsh[1]\n",
    "    wi = xdsh[2]\n",
    "    c = xdsh[3]\n",
    "\n",
    "    f = self.fc(xd) # [bs, h, w, c']\n",
    "    g = self.gc(xd) # [bs, h, w, c']\n",
    "    h = self.hc(xd) # [bs, h, w, c]\n",
    "    inputs = self.xc(inputs)\n",
    "    \n",
    "    f = tf.reshape(f, [-1, hi*wi, ch])\n",
    "    g = tf.reshape(g, [-1, hi*wi, ch])\n",
    "    h = tf.reshape(h, [-1, hi*wi, ch])\n",
    "\n",
    "    s = tf.matmul(g, f, transpose_b=True) # # [bs, N, N]\n",
    "\n",
    "    beta = tf.nn.softmax(s)  # attention map\n",
    "\n",
    "    o = tf.matmul(beta, h) # [bs, N, C]\n",
    "    \n",
    "    o = tf.reshape(o, shape=[b, hi, wi, tf.cast(ch, tf.int32)])\n",
    "    o = self.upscale1(o)\n",
    "    o = self.upscale2(o)\n",
    "    \n",
    "\n",
    "    x = self.gamma * o + inputs\n",
    "\n",
    "    if self.use_batch_norm:\n",
    "        x = self.norm(x)\n",
    "\n",
    "    if self.activation:\n",
    "        x = self.act(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "\n",
    "def get_conv(filters, use_attention=False, use_batch_norm=False):\n",
    "    conv = tf.keras.Sequential()\n",
    "    if use_attention:\n",
    "        conv.add(AttentionLayer(filters, use_batch_norm=use_batch_norm))\n",
    "    else:\n",
    "        if use_batch_norm:\n",
    "            conv.add(layers.Conv2D(filters, 3, padding=\"same\", use_bias=False))\n",
    "            conv.add(layers.BatchNormalization(momentum=0.3))\n",
    "        else:\n",
    "            conv.add(tfa.layers.SpectralNormalization(\n",
    "                layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "            ))\n",
    "        \n",
    "    conv.add(layers.LeakyReLU())\n",
    "    return conv\n",
    "\n",
    "def get_conv_t(filters, use_batch_norm=False):\n",
    "    conv = tf.keras.Sequential()\n",
    "\n",
    "    if use_batch_norm:\n",
    "        conv.add(layers.Conv2DTranspose(filters, 3, strides=2, padding=\"same\", use_bias=False))\n",
    "        conv.add(layers.BatchNormalization(momentum=0.3))\n",
    "    else:\n",
    "        conv.add(tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2DTranspose(filters, 3, strides=2, padding=\"same\", use_bias=False)\n",
    "        ))\n",
    "        \n",
    "    conv.add(layers.LeakyReLU())\n",
    "    return conv\n",
    "    \n",
    "def get_encoder(filters=[16,64,128,256,512], use_attention=False, use_batch_norm=False):\n",
    "    encoder = tf.keras.Sequential(name=\"encoder\")\n",
    "    # encoder.add(layers.GaussianDropout(0.2))\n",
    "    encoder.add(get_conv(filters[0], use_batch_norm))\n",
    "    for f in filters[1:]:\n",
    "        encoder.add(get_conv(f, use_batch_norm=use_batch_norm))\n",
    "        encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "  \n",
    "    encoder.add(get_conv(filters[-1], use_attention, use_batch_norm))\n",
    "    return encoder\n",
    "\n",
    "def get_decoder(filters=[256,128, 64,16], use_attention=False, use_batch_norm=False):\n",
    "    decoder = tf.keras.Sequential(name=\"decoder\")\n",
    "    \n",
    "    for f in filters:\n",
    "        decoder.add(get_conv_t(f, use_batch_norm=use_batch_norm))\n",
    "    \n",
    "    if use_attention:\n",
    "        decoder.add(AttentionLayer(filters[IMG_CHANNELS]))\n",
    "    decoder.add(layers.Conv2D(IMG_CHANNELS, 3, padding='same', activation='tanh'))\n",
    "    return decoder\n",
    "\n",
    "class AutoEncoder(tf.keras.Model):\n",
    "  def __init__(self, encoder_conf, decoder_conf, use_attention, use_batch_norm=False):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    self.encoder_conf = encoder_conf\n",
    "    self.decoder_conf = decoder_conf\n",
    "    self.use_attention = use_attention\n",
    "    self.use_batch_norm = use_batch_norm\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.encoder = get_encoder(self.encoder_conf, use_attention=self.use_attention, use_batch_norm=self.use_batch_norm)\n",
    "    self.decoder = get_decoder(self.decoder_conf, use_attention=self.use_attention, use_batch_norm=self.use_batch_norm)\n",
    "    \n",
    "    self.encoder.build(input_shape=input_shape)\n",
    "    \n",
    "    sh = self.encoder.output_shape\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    self.seq1 = layers.Dense(1024)\n",
    "    self.reshape = layers.Reshape([*sh[1:]])\n",
    "    \n",
    "    self.last = layers.Conv2D(IMG_CHANNELS, 3, padding='same', activation='tanh')\n",
    "    self.inputs_dropout = layers.Dropout(0.2)\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.inputs_dropout(inputs)\n",
    "    x = self.encoder(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.reshape(x)\n",
    "    x = self.decoder(x)\n",
    "    \n",
    "    # x_inputs = self.conv_input(inputs)\n",
    "    \n",
    "    # x = self.add([x * self.gamma, x_inputs])\n",
    "    # x = self.attention(x)\n",
    "    x = self.last(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = next(iter(x_train.map(normalize).batch(1)))\n",
    "\n",
    "print(y.shape)\n",
    "\n",
    "enc = get_encoder([16,64,128,256,512])(y)\n",
    "print(enc.shape)\n",
    "\n",
    "dec = get_decoder([256,128,64,14])(enc)\n",
    "\n",
    "print(dec.shape)\n",
    "\n",
    "assert y.shape == dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_images = []\n",
    "\n",
    "# def train_get():\n",
    "#    for x, y in zip(x_train, y_train):\n",
    "#         x = tf.expand_dims(x, axis=2)\n",
    "#         generated_images.append(y)\n",
    "#         yield x\n",
    "\n",
    "# def test_get():\n",
    "#    for x in x_test:\n",
    "#         x = tf.expand_dims(x, axis=2)\n",
    "#         yield x\n",
    "\n",
    "# output_signature=tf.TensorSpec(shape=(28, 28, 1), dtype=tf.float32)\n",
    "\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "def get_train_ds():\n",
    "    return x_train.map(normalize, num_parallel_calls=tf.data.AUTOTUNE).map(lambda x: (x,x)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "#     return tf.data.Dataset.from_generator(train_get, output_signature=output_signature).map(lambda x: (x,x)).batch(BATCH_SIZE)\n",
    "\n",
    "def get_test_ds():\n",
    "    return x_test.map(normalize, num_parallel_calls=tf.data.AUTOTUNE).map(lambda x: (x,x)).batch(5).prefetch(tf.data.AUTOTUNE)\n",
    "#     return tf.data.Dataset.from_generator(test_get, output_signature=output_signature).map(lambda x: (x,x)).take(100).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/minst/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSIM(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='ssim', **kwargs):\n",
    "    super(SSIM, self).__init__(name=name, **kwargs)\n",
    "    self.ssim = self.add_weight(name='ssim', initializer='zeros')\n",
    "    self.ep = 0.0000001\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = denormalize(y_true)\n",
    "    y_pred = denormalize(y_pred)\n",
    "    same = tf.math.reduce_sum(tf.image.ssim(y_true, y_true, 255.0, filter_size=3)) + self.ep\n",
    "    values = (self.ep + tf.math.reduce_sum(tf.image.ssim(y_true, y_pred, 255.0, filter_size=3))) / same\n",
    "    \n",
    "    values = tf.cast(values, self.dtype)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "        sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "        values = tf.multiply(values, sample_weight)\n",
    "    self.ssim.assign(tf.reduce_sum(values))\n",
    "\n",
    "  def result(self):\n",
    "    return self.ssim\n",
    "\n",
    "class SSIM_Multiscale(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='ssim_ms', **kwargs):\n",
    "    super(SSIM_Multiscale, self).__init__(name=name, **kwargs)\n",
    "    self.ssim_ms = self.add_weight(name='ssim_ms', initializer='zeros')\n",
    "    self.self_ssim_ms = self.add_weight(name='self_ssim_ms', initializer='zeros')\n",
    "    self.ep = 0.0000001\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = denormalize(y_true)\n",
    "    y_pred = denormalize(y_pred)\n",
    "    same = tf.math.reduce_sum(tf.image.ssim_multiscale(y_true, y_true, 255.0, filter_size=3)) + self.ep\n",
    "    values = (self.ep + tf.math.reduce_sum(tf.image.ssim_multiscale(y_true, y_pred, 255.0, filter_size=4))) / same\n",
    "\n",
    "    values = tf.cast(values, self.dtype)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "        sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "        values = tf.multiply(values, sample_weight)\n",
    "    self.ssim_ms.assign(values)\n",
    "    self.self_ssim_ms.assign(same)\n",
    "\n",
    "  def result(self):\n",
    "    return self.ssim_ms\n",
    "\n",
    "class TOP_SSIM_Multiscale(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='self_ssim_ms', **kwargs):\n",
    "    super(TOP_SSIM_Multiscale, self).__init__(name=name, **kwargs)\n",
    "    self.self_ssim_ms = self.add_weight(name='self_ssim_ms', initializer='zeros')\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = denormalize(y_true)\n",
    "    values = tf.math.reduce_sum(tf.image.ssim_multiscale(y_true, y_true, 255.0, filter_size=3))\n",
    "\n",
    "    values = tf.cast(values, self.dtype)\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = tf.cast(sample_weight, self.dtype)\n",
    "        sample_weight = tf.broadcast_to(sample_weight, values.shape)\n",
    "        values = tf.multiply(values, sample_weight)\n",
    "    self.self_ssim_ms.assign(values)\n",
    "\n",
    "  def result(self):\n",
    "    return self.self_ssim_ms\n",
    "\n",
    "\n",
    "class DenormalizedMSE(tf.keras.metrics.Metric):\n",
    "\n",
    "  def __init__(self, name='denormalized_mse', **kwargs):\n",
    "    super(DenormalizedMSE, self).__init__(name=name, **kwargs)\n",
    "    self.denormalized_mse = self.add_weight(name='denormalized_mse', initializer='zeros')\n",
    "\n",
    "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "    y_true = denormalize(y_true)\n",
    "    y_pred = denormalize(y_pred)\n",
    "    self.mse = keras.losses.MeanSquaredError()\n",
    "\n",
    "    self.denormalized_mse.assign(self.mse(y_true, y_pred))\n",
    "\n",
    "  def result(self):\n",
    "    return self.denormalized_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "for conf in experiments:    \n",
    "    model_name = f'coli100_{EXPERIMENT_VERSION}_{conf.get_name()}'\n",
    "    train_log_dir = f'logs/minst/{model_name}'\n",
    "    if os.path.exists(train_log_dir):\n",
    "        shutil.rmtree(train_log_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyModel(tf.keras.Model):\n",
    "#   def __init__(self, autoencoder):\n",
    "#     super(MyModel, self).__init__()\n",
    "#     self.autoencoder = autoencoder\n",
    "\n",
    "#   def call(self, inputs, training=False):\n",
    "#     return self.autoencoder(inputs, training=training)\n",
    "\n",
    "  # def validation_step(self, images):\n",
    "  #   pass\n",
    "\n",
    "  # def train_step(self, images):\n",
    "  #   with tf.GradientTape() as auto_tape:\n",
    "  #     generated = self.autoencoder(images)\n",
    "  #     loss = cross_entropy(images, generated)\n",
    "  #   gradients = auto_tape.gradient(loss, self.autoencoder.trainable_variables)\n",
    "  #   opt.apply_gradients(zip(gradients, self.autoencoder.trainable_variables))\n",
    "\n",
    "  #   return {\"loss\": loss}\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for conf in experiments:    \n",
    "    model_name = f'coli100_{EXPERIMENT_VERSION}_{conf.get_name()}'\n",
    "    train_log_dir = f'logs/minst/{model_name}'\n",
    "\n",
    "    \n",
    "    tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = train_log_dir,\n",
    "      write_graph=True,\n",
    "      histogram_freq = 1,\n",
    "      update_freq=\"batch\"\n",
    "      )\n",
    "\n",
    "    \n",
    "    \n",
    "    EPOCHS = 60\n",
    "    \n",
    "    denorm = conf.use_denorm\n",
    "\n",
    "    model = conf.create_model()\n",
    "\n",
    "\n",
    "    class SkMetrics(keras.callbacks.Callback):\n",
    "        def on_train_begin(self, logs={}):\n",
    "            tf.summary.text(\"description\", str(conf))\n",
    "            self.validation_loss = []\n",
    "            self.epoch_n = 0\n",
    "#             self.batch_ssim = []\n",
    "#             self.batch_ssim_ms = []\n",
    "#             self.batch_denormalized_mse = []\n",
    "            \n",
    "        def on_epoch_end(self, epoch, logs):\n",
    "            self.epoch_n = epoch\n",
    "#             tf.summary.scalar('ssim', logs[\"ssim\"])\n",
    "#             tf.summary.scalar('ssim_ms', logs[\"ssim_ms\"])\n",
    "#             tf.summary.scalar('denormalized_mse', logs[\"denormalized_mse\"])\n",
    "\n",
    "        def on_batch_end(self, batch, logs={}):\n",
    "          def expand_and_predict(x):\n",
    "            result = model(x, training=False)\n",
    "            return denormalize(result)\n",
    "          if batch % 99 == 0 and batch > 0:\n",
    "            display.clear_output(wait=True)\n",
    "            p = f'./minst_output/{model_name}/'\n",
    "            isExist = os.path.exists(p)\n",
    "            if not isExist:\n",
    "                os.makedirs(p)\n",
    "            full_path = f'./minst_output/{model_name}/{self.epoch_n}_{batch}'\n",
    "            print_validation(expand_and_predict, save=True, path=full_path)\n",
    "            print(conf.get_name())\n",
    "\n",
    "\n",
    "    class CustomMSE(keras.losses.Loss):\n",
    "        def __init__(self, denorm=True, name=\"custom_mse\"):\n",
    "            super().__init__(name=name)\n",
    "            self.mse = keras.losses.MeanSquaredError()\n",
    "            self.denorm = denorm\n",
    "\n",
    "        def call(self, y_true, y_pred):\n",
    "            if denorm:\n",
    "                return self.mse(denormalize(y_true), denormalize(y_pred))\n",
    "            else:\n",
    "                return self.mse(y_true, y_pred)\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "    # model.compile(run_eagerly=True)\n",
    "    model.compile(loss=CustomMSE(denorm=denorm), optimizer=opt,\n",
    "                  metrics=[\"mse\", DenormalizedMSE(), SSIM_Multiscale(), SSIM()])\n",
    "#                   run_eagerly=True\n",
    "#                  )\n",
    "    print(conf.get_name())\n",
    "    history = model.fit(get_train_ds().repeat(), epochs=60, steps_per_epoch=100, validation_data=get_test_ds(), callbacks=[SkMetrics(), tboard_callback])\n",
    "    conf.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(generated_images), 5*5*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import pathlib\n",
    "\n",
    "for conf in experiments:    \n",
    "    model_name = f'./minst_output/coli100_{EXPERIMENT_VERSION}_{conf.get_name()}'\n",
    "    data_dir = pathlib.Path(model_name)\n",
    "    pictures = list(data_dir.glob('*.png'))\n",
    "    pictures.sort()\n",
    "    with imageio.get_writer(f'{model_name}.gif', mode='I') as writer:\n",
    "        for filename in pictures:\n",
    "            image = imageio.imread(filename)\n",
    "            writer.append_data(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2",
   "language": "python",
   "name": "ml2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4327198819dafd55a2243f22aba11bf2a7d9f0c32aced8ba7d18a900e49d0553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
