{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export XLA_FLAGS=--xla_gpu_cuda_data_dir=/usr/lib/cuda\n",
    "!export CUDA_DIR=\"/usr/lib/cuda\"\n",
    "!export TF_GPU_ALLOCATOR=cuda_malloc_async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.threading.set_inter_op_parallelism_threads(0)\n",
    "tf.config.threading.set_intra_op_parallelism_threads(0)\n",
    "# physical_devices = tf.config.list_physical_devices('GPU')\n",
    "# tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8000)])\n",
    "# tf.config.experimental.set_virtual_device_configuration(physical_devices[1], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2000)])\n",
    "# for gpu in physical_devices:\n",
    "#     tf.config.experimental.set_memory_growth(gpu, True)\n",
    "# print(\"GPUS: {}\".format(len(physical_devices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "layers = tf.keras.layers\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
    "# train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
    "# train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
    "# !rm -rf ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    a = (image - 127.5) \n",
    "    return a / 127.5\n",
    "\n",
    "def denormalize(image):\n",
    "    # return image\n",
    "    return tf.cast(tf.cast(image, tf.float32) * 127.5 + 127.5, np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_w = 720\n",
    "input_h = 480\n",
    "\n",
    "input_w = input_w / 2\n",
    "input_h = input_h / 2\n",
    "\n",
    "sub_scale = 1/2\n",
    "\n",
    "# input_h = 672\n",
    "# input_w = 976\n",
    "\n",
    "# input_h *= 1.2\n",
    "# input_w *= 1.2\n",
    "\n",
    "\n",
    "# input_h = 576\n",
    "# input_w = 864\n",
    "\n",
    "hwfactor = input_h / input_w\n",
    "\n",
    "input_h, input_w\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "EPOCHS=1000\n",
    "STEPS_PER_EPOCH = 900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h = int(input_h)\n",
    "input_w = int(input_w)\n",
    "input_h, input_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enlarge image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = 1\n",
    "resize_factor = 1/rf # about 0.0833\n",
    "resized_size_h = int(input_h * resize_factor)\n",
    "resized_size_w = int(input_w * resize_factor)\n",
    "\n",
    "print(resized_size_h, resized_size_h * rf, input_h)\n",
    "assert resized_size_h * rf == input_h\n",
    "\n",
    "print(resized_size_w, resized_size_w * rf, input_w)\n",
    "assert resized_size_w * rf == input_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and save random images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def image_generator(pictures = [], large_pictures = [], ds_pictures = [], return_ext=False):\n",
    "    random.shuffle(pictures)\n",
    "    def parse_name(filename):\n",
    "        return \"{}_\" + re.sub(r\"\\.(png|jpeg|jpg)\", r\"_{}.\\1\", os.path.basename(filename))\n",
    "    \n",
    "    def format_name(name, subname):\n",
    "        return name.format(random.randint(0, 1000), subname)\n",
    "    def fn():\n",
    "        input_w = 720\n",
    "        input_h = 480\n",
    "        for filename in pictures:\n",
    "            fname = parse_name(filename)\n",
    "            raw_png = tf.io.read_file(str(filename), name=filename)\n",
    "            decoded_png_2 = tf.image.decode_png(raw_png, channels=3, name=filename)\n",
    "            decoded_png_2 = tf.image.resize(decoded_png_2, [input_h, input_w],\n",
    "                              method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "            yield decoded_png_2, format_name(fname, \"original\")\n",
    "            yield tf.image.flip_left_right(decoded_png_2), format_name(fname, \"flipped\")\n",
    "\n",
    "        for filename in ds_pictures:\n",
    "            fname =  parse_name(filename)\n",
    "            raw_png = tf.io.read_file(str(filename), name=filename)\n",
    "            decoded_png = tf.image.decode_jpeg(raw_png, channels=3, name=filename)\n",
    "            cropped = tf.image.resize_with_crop_or_pad(\n",
    "              decoded_png,input_h ,input_w\n",
    "            )\n",
    "            yield cropped, format_name(fname, \"original\")\n",
    "            yield tf.image.flip_left_right(cropped), format_name(fname, \"center_crop\")\n",
    "\n",
    "            for i in range(200):\n",
    "                cropped = tf.image.random_crop(\n",
    "                  decoded_png, size=[input_h, input_w, 3])\n",
    "                yield cropped, format_name(fname, \"random_crop_\"+str(i))\n",
    "                yield tf.image.flip_left_right(cropped), format_name(fname, \"random_crop_and_flip_\"+str(i))\n",
    "\n",
    "        for filename in large_pictures:\n",
    "            fname =  parse_name(filename)\n",
    "            raw_png = tf.io.read_file(str(filename), name=filename)\n",
    "            decoded_png = tf.image.decode_jpeg(raw_png, channels=3, name=filename)\n",
    "            cropped = tf.image.resize_with_crop_or_pad(\n",
    "              decoded_png,input_h ,input_w\n",
    "            )\n",
    "            yield cropped, format_name(fname, \"center_crop\")\n",
    "\n",
    "            for i in range(20):\n",
    "                cropped = tf.image.random_crop(\n",
    "                  decoded_png, size=[input_h, input_w, 3])\n",
    "                yield cropped, format_name(fname, \"random_crop_\"+str(i))\n",
    "            # yield tf.image.random_flip_left_right(cropped)\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from multiprocessing import Lock, Process, Queue, current_process, Value, Pool, cpu_count\n",
    "from concurrent import futures\n",
    "\n",
    "def cache_images():\n",
    "    data_dir = pathlib.Path(\"./prepa\")\n",
    "    pictures = list(data_dir.glob('*.png'))\n",
    "\n",
    "    # TODO: Remove filter\n",
    "    pictures = random.sample(pictures, 1000)\n",
    "\n",
    "\n",
    "    data_dir = pathlib.Path(\"./people\")\n",
    "    large_pictures = list(data_dir.glob('*.jpg'))\n",
    "\n",
    "\n",
    "    data_dir = pathlib.Path(\"./ds_images\")\n",
    "    ds_pictures = list(data_dir.glob('*.jpg'))\n",
    "    \n",
    "    approx_size = len(pictures)*2 + len(ds_pictures)*2 + len(ds_pictures)*200*2 + len(large_pictures) + len(large_pictures)*20\n",
    "    \n",
    "    gen = image_generator(pictures, large_pictures, ds_pictures)\n",
    "    \n",
    "    \n",
    "    print(\"starting\")\n",
    "    counter = Value('i', 0)\n",
    "    \n",
    "        \n",
    "    it = iter(gen())\n",
    "    con = True\n",
    "    \n",
    "    np = cpu_count()\n",
    "    print(f'You have {np} cores')\n",
    "\n",
    "\n",
    "\n",
    "    def f1(im, filename):\n",
    "            im.numpy()\n",
    "        #     input_w = 720\n",
    "        # input_h = 480\n",
    "            tf.keras.utils.save_img('./tmp/{}'.format(filename), im)\n",
    "            with counter.get_lock():\n",
    "                counter.value += 1\n",
    "\n",
    "            if (counter.value % 10 == 0):\n",
    "                clear_output(wait=True)\n",
    "                print(f\"{counter.value+1}/{approx_size}\")   \n",
    "            return f\"{im.shape} {filename}\"\n",
    "    with futures.ThreadPoolExecutor(max_workers=min(int((np/3) * 2), 16)) as executor:    \n",
    "        for im, filename  in it:\n",
    "            # print(im.shape)\n",
    "            assert (im.shape[0] == 480 and im.shape[1] == 720)\n",
    "#             im, filename = next(it)\n",
    "            future = executor.submit(f1, im, filename)\n",
    "#             future.add_done_callback(lambda x: print(f\"donee: {x}\"))\n",
    "        try:\n",
    "            data = future.result()\n",
    "        except Exception as exc:\n",
    "            print('generated an exception: %s' % exc)\n",
    "        else:\n",
    "            print('%r page is' % data)\n",
    "    \n",
    "\n",
    "# cache_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the cached images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def image_generator_cached(size=None):\n",
    "    data_dir = pathlib.Path(\"./tmp\")\n",
    "    # prepa_dir = pathlib.Path(\"./prepa\")\n",
    "    prepa_files = list(data_dir.glob('*.png'))\n",
    "    random.shuffle(prepa_files)\n",
    "    all_files = []\n",
    "    for filename in prepa_files:\n",
    "        all_files.append((filename, 'png'))\n",
    "    for filename in list(data_dir.glob('*.jpg')):\n",
    "        all_files.append((filename, 'jpg'))\n",
    "    \n",
    "    random.shuffle(all_files)\n",
    "    if size:\n",
    "        all_files = all_files[:size]\n",
    "    \n",
    "    def fn():\n",
    "        for filename, img_type in all_files:\n",
    "            raw_image = tf.io.read_file(str(filename), name=filename)\n",
    "            if img_type == \"png\":\n",
    "                yield tf.image.decode_png(raw_image, channels=3, name=filename)\n",
    "            elif img_type == \"jpg\":\n",
    "                yield tf.image.decode_jpeg(raw_image, channels=3, name=filename)\n",
    "            \n",
    "    return fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_count = len(list(pathlib.Path(\"./tmp\").glob('*')))\n",
    "print(f\"image_count={image_count}\")\n",
    "\n",
    "fraction = int((STEPS_PER_EPOCH * EPOCHS) / image_count)\n",
    "print(fraction)\n",
    "\n",
    "dataset_size = int((STEPS_PER_EPOCH * EPOCHS) / (fraction + 1))\n",
    "print(f\"better dataset size: {dataset_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_h, input_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downsize_image_ratio = 1/4\n",
    "\n",
    "\n",
    "def make_dataset(size=None):\n",
    "    load_large_images = image_generator_cached(size)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_generator(load_large_images,  output_signature=\n",
    "         tf.TensorSpec(shape=(int(input_h/sub_scale), int(input_w/sub_scale), 3), dtype=tf.float32))\n",
    "            # .map(normalize)\n",
    "\n",
    "    def resize_and_couple(images):\n",
    "        images = normalize(images)\n",
    "        images = tf.image.resize(images, (input_h, input_w))\n",
    "        \n",
    "        # cropped = tf.image.resize_with_crop_or_pad(\n",
    "#               decoded_png,input_h ,input_w\n",
    "#             )\n",
    "        print(f\"images.shape={images.shape}\")\n",
    "        if random.choices([True,False], weights=(0.3,0.7), k=1)[0] == True:\n",
    "            return (images, images)\n",
    "        \n",
    "        # return (images,images)\n",
    "        down = tf.image.resize(\n",
    "            images,\n",
    "            [int(resized_size_h * downsize_image_ratio), int(resized_size_w * downsize_image_ratio)],\n",
    "            preserve_aspect_ratio=True,\n",
    "            antialias=False,\n",
    "            name=None)\n",
    "\n",
    "        up = tf.image.resize(\n",
    "            down,\n",
    "            [resized_size_h, resized_size_w],\n",
    "            preserve_aspect_ratio=True,\n",
    "            antialias=False,\n",
    "            name=None)\n",
    "        \n",
    "        \n",
    "\n",
    "        return (images, up)\n",
    "\n",
    "    # zipped_train_dataset = train_ds.interleave(\n",
    "    #   lambda x: tf.data.Dataset.from_tensors(x).map(resize_and_couple, num_parallel_calls=tf.data.AUTOTUNE),\n",
    "    #   cycle_length=4, num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    #   deterministic=False\n",
    "    # )\n",
    "    zipped_train_dataset = train_ds.map(resize_and_couple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    # Batch and shuffle the data\n",
    "    return zipped_train_dataset\n",
    "    # train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(1)\n",
    "\n",
    "# lp_imgs = list(train_ds.shuffle(200).take(2))\n",
    "train_ds = make_dataset(dataset_size).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_samples(size=2):\n",
    "    return make_dataset(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_size_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input_w = input_w\n",
    "model_input_h = input_h\n",
    "\n",
    "model_input_w, model_input_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_size_w, resized_size_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_w, input_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(stride, base_kernel, K):\n",
    "    lst = list(range(0,base_kernel+stride, stride))\n",
    "    return lst[min(range(len(lst)), key = lambda i: abs(lst[i]-K))]\n",
    "\n",
    "closest(2, 6, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_grid_size(x):\n",
    "    col = int(math.sqrt(x))\n",
    "    row = int(x / col)\n",
    "\n",
    "    y = int(x - (col*row))\n",
    "    row+=y\n",
    "\n",
    "    return row, col\n",
    "\n",
    "def get_bi_column(x, col=2):\n",
    "    row = int(x / col)\n",
    "\n",
    "    y = int(x - (col*row))\n",
    "    row+=y\n",
    "\n",
    "    return row, col\n",
    "\n",
    "get_bi_column(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_generator(generator, discriminator, autoencoder, size=2):\n",
    "    # test_dataset = tf.data.Dataset.zip((train_ds, resized_ds))\n",
    "    random_samples_ds = get_random_samples(size)\n",
    "    test_data = random_samples_ds.batch(size)\n",
    "    for images, resized_images in test_data:\n",
    "        generated_images = generator(resized_images, training=False)\n",
    "        encoded = autoencoder(images, training=False)\n",
    "\n",
    "#         den_generated_images = denormalize(generated_images)\n",
    "\n",
    "        imgs = zip(images, resized_images, encoded, generated_images)\n",
    "\n",
    "        col, row = get_bi_column(size*4, 4)\n",
    "\n",
    "        # print(f\"col={col}, row={row}\")\n",
    "\n",
    "        fig = plt.figure(figsize=(row * 10, col * 7))\n",
    "\n",
    "        def render_image(image, i):\n",
    "            img = denormalize(image)\n",
    "            plt.subplot(col, row, i+1)\n",
    "            # im = np.copy(img)\n",
    "            plt.imshow(tf.cast(img, np.uint8))\n",
    "            plt.axis('off')\n",
    "\n",
    "        i = 0\n",
    "        for original, resize, enc, gener in imgs:\n",
    "            render_image(original, i)\n",
    "            render_image(resize, i+1)\n",
    "            render_image(enc, i+2)\n",
    "            render_image(gener, i+3)\n",
    "\n",
    "        decision = discriminator(generated_images, training=False)\n",
    "        print(f\"Decision shape: {decision.shape}\")\n",
    "        print (f\"Decision for the scaled images: {decision}\")\n",
    "\n",
    "\n",
    "    return generated_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vgg19 = VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")\n",
    "vgg19.trainable = False\n",
    "\n",
    "# vgg19 = tf.keras.Model(\n",
    "#     inputs=model.input,\n",
    "#     outputs=model.output\n",
    "# )\n",
    "\n",
    "\n",
    "# model = VGG19(\n",
    "#     include_top=False,\n",
    "#     weights='imagenet',\n",
    "# )\n",
    "# model.trainable = False\n",
    "\n",
    "def load_and_process_image(image_path):\n",
    "    img = load_img(image_path)\n",
    "    # convert image to array\n",
    "    img = img_to_array(img)\n",
    "    img = preprocess_input(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def deprocess(img):\n",
    "    # perform the inverse of the pre processing step\n",
    "    i = np.copy(img)\n",
    "    i[:, :, 0] += 103.939\n",
    "    i[:, :, 1] += 116.779\n",
    "    i[:, :, 2] += 123.68\n",
    "    # convert RGB to BGR\n",
    "    i = i[:, :, ::-1]\n",
    " \n",
    "    i = np.clip(i, 0, 255).astype('uint8')\n",
    "    return i\n",
    " \n",
    " \n",
    "def display_image(img):\n",
    "    # remove one dimension if image has 4 dimension\n",
    "    if len(img.shape) == 4:\n",
    "        img = np.squeeze(img, axis=0)\n",
    " \n",
    "    img = deprocess(img)\n",
    " \n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(img)\n",
    "    return\n",
    "\n",
    "\n",
    "# def get_content_model():\n",
    "#     content_layer = 'block5_conv2'\n",
    "#     content_model = tf.keras.Model(\n",
    "#         inputs=model.input,\n",
    "#         outputs=model.get_layer(content_layer).output\n",
    "#     )\n",
    "#     # content_model.summary()\n",
    "#     return content_model\n",
    "\n",
    "# c_model = get_content_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 480 / 15, 720 / 22, 32 * 32\n",
    "480 / 15, 704 / 22, 720 / 704\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inx = 15\n",
    "# iny = 22\n",
    "\n",
    "# ex = 480\n",
    "# ey = 720\n",
    "\n",
    "# x * 15 = 480\n",
    "# y * 22 = 720\n",
    "\n",
    "# (x * 15) + (y * 22) = 480 + 720\n",
    "# (x * 15) + (y * 22) - 480 - 720"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "480 / 30, 720 / 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg19.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# content_layer = 'block5_conv2'\n",
    "# content_model = tf.keras.Model(\n",
    "#     inputs=vgg19.input,\n",
    "#     outputs=vgg19.get_layer(content_layer).output\n",
    "# )\n",
    "# style_layers = [\n",
    "#     'block1_conv1',\n",
    "#     'block3_conv1',\n",
    "#     'block5_conv1'\n",
    "# ]\n",
    "# style_models = [tf.keras.Model(inputs=vgg19.input,\n",
    "#                       outputs=vgg19.get_layer(layer).output) for layer in style_layers]\n",
    "\n",
    "\n",
    "# def content_loss(content, generated):\n",
    "#     a_C = content_model(content)\n",
    "#     a_G = content_model(generated)\n",
    "#     loss = tf.reduce_mean(tf.square(a_C - a_G))\n",
    "#     return loss\n",
    "\n",
    "# def gram_matrix(A):\n",
    "#     channels = int(A.shape[-1])\n",
    "#     a = tf.reshape(A, [-1, channels])\n",
    "#     n = tf.shape(a)[0]\n",
    "#     gram = tf.matmul(a, a, transpose_a=True)\n",
    "#     return gram / tf.cast(n, tf.float32)\n",
    " \n",
    " \n",
    "# weight_of_layer = 1. / len(style_models)\n",
    " \n",
    " \n",
    "# # style loss\n",
    "# def style_cost(style, generated):\n",
    "#     J_style = 0\n",
    " \n",
    "#     for style_model in style_models:\n",
    "#         a_S = style_model(style)\n",
    "#         a_G = style_model(generated)\n",
    "#         GS = gram_matrix(a_S)\n",
    "#         GG = gram_matrix(a_G)\n",
    "#         current_cost = tf.reduce_mean(tf.square(GS - GG))\n",
    "#         J_style += current_cost * weight_of_layer\n",
    " \n",
    "#     return J_style\n",
    "\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# a=1\n",
    "# b=100\n",
    "# @tf.function\n",
    "# def train_step(images, generated, iterations=50):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         J_content = content_loss(images, generated)\n",
    "#         J_style = style_cost(images, generated)\n",
    "#         J_total = a * J_content + b * J_style\n",
    "\n",
    "#     grads = tape.gradient(J_total, generated)\n",
    "#     opt.apply_gradients([(grads, generated)])\n",
    "\n",
    "#     return generated, J_total\n",
    "\n",
    "# # layers.Input()\n",
    "        \n",
    "\n",
    "# for d in get_random_samples(1).take(1).batch(1):\n",
    "#     images, resized  = d\n",
    "\n",
    "#     best_cost = 9e13\n",
    "#     best_image = None\n",
    "#     rresized = tf.Variable(tf.identity(resized), trainable=True, dtype=tf.float32)\n",
    "#     plt.figure()\n",
    "#     display_image(images)\n",
    "#     for i in range(100):\n",
    "#         generated, cost = train_step(images, rresized)\n",
    "#         best_cost = tf.math.minimum(best_cost, cost)\n",
    "#         if best_cost == cost:\n",
    "#             best_image = tf.identity(generated)\n",
    "#             plt.figure()\n",
    "#             display_image(best_image)\n",
    "#         if i % 10 == 0:\n",
    "#             print(\"Iteration :{}\".format(i))\n",
    "#             print('Total Loss {:e}.'.format(cost))\n",
    "#     display_image(best_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content_path = \"tmp/0_159-Pre-registro-bachillerato_1_random_crop_168.jpg\"\n",
    "# content_img = load_and_process_image(content_path)\n",
    "# display_image(content_img)\n",
    "\n",
    "content_model = tf.keras.Model(\n",
    "    inputs=vgg19.input,\n",
    "    outputs=vgg19.get_layer(\"block4_conv4\").output\n",
    ")\n",
    "content_model.trainable = False\n",
    "\n",
    "def content_loss(content, generated):\n",
    "    a_C = content_model(preprocess_input(content))\n",
    "    a_G = content_model(preprocess_input(generated))\n",
    "    loss = tf.reduce_mean(tf.square(a_C - a_G))\n",
    "    return loss\n",
    "\n",
    "def build_gen_contents(inputs, pre_trained_model):\n",
    "    x = pre_trained_model(inputs)\n",
    "\n",
    "    scale = 4\n",
    "    x = layers.Conv2D(256*3, 1, padding=\"same\", activation=\"leaky_relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # x = layers.BatchNormalization()(x)\n",
    "    x = tf.nn.depth_to_space(x, scale)\n",
    "\n",
    "\n",
    "    y1 = layers.Conv2D(48, 3, padding=\"same\", strides=2, activation=\"leaky_relu\")(inputs)\n",
    "\n",
    "    x = layers.Add()([y1,x]) \n",
    "\n",
    "    x = tf.nn.depth_to_space(x, 2)\n",
    "    y2 = layers.Conv2D(12, 3, padding=\"same\", activation=\"leaky_relu\")(inputs)\n",
    "    x = layers.Add()([y2,x]) \n",
    "\n",
    "    x = layers.Conv2D(3,\n",
    "        kernel_size=9,\n",
    "        padding='same',\n",
    "        # activation=\"tanh\"\n",
    "    )(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# x = build_gen_contents(content_img, model)\n",
    "# plt.figure()\n",
    "# display_image(x)\n",
    "\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_kernel(s):\n",
    "        return closest(min(2,s), s, s*hwfactor), s\n",
    "\n",
    "# init_fn = tf.keras.initializers.LecunUniform(seed=123)\n",
    "# init_fn = tf.keras.initializers.GlorotNormal()\n",
    "\n",
    "def residual_block(block_input, filters=64, momentum=0.8):\n",
    "#     x = layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn)(block_input)\n",
    "    x = tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, kernel_size=3, padding='same'))(block_input)\n",
    "#     x = tfa.layers.SpectralNormalization(\n",
    "#         layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn))(block_input)\n",
    "#     x = layers.BatchNormalization(momentum=momentum)(x)\n",
    "\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "#     x = tfa.layers.SpectralNormalization(\n",
    "#         layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn))(x)\n",
    "    x = tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, kernel_size=3, padding='same'))(x)\n",
    "#     x = layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn)(x)\n",
    "#     x = layers.BatchNormalization(momentum=momentum)(x)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    x = layers.Add()([block_input,x]) \n",
    "#     x = tfa.layers.InstanceNormalization(axis=3, center=True, scale=True, \n",
    "#                                          beta_initializer=\"random_uniform\", gamma_initializer=\"random_uniform\")(x)\n",
    "#     x = tfa.layers.GroupNormalization(groups=8, axis=3)(x)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    return x\n",
    "\n",
    "def pixel_shuffle(x, channels, downsampleFactor, momentum=0.8):\n",
    "    if downsampleFactor == 1:\n",
    "#         x = layers.Conv2D(channels * (downsampleFactor ** 2), 3, padding=\"same\",\n",
    "#                activation=\"relu\", kernel_initializer=\"Orthogonal\",\n",
    "#                          strides=2)(x)\n",
    "#         x = layers.BatchNormalization(momentum=momentum)(x)\n",
    "        x = tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2D(channels * 2, 3, padding=\"same\",\n",
    "               activation=\"leaky_relu\", kernel_initializer=\"Orthogonal\",\n",
    "                         strides=2))(x)\n",
    "        outputs = tf.nn.depth_to_space(x, 2)\n",
    "    else:\n",
    "        x = tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2D(channels * (downsampleFactor * 2), 3, padding=\"same\",\n",
    "               activation=\"relu\", kernel_initializer=\"Orthogonal\"))(x)\n",
    "#         x = layers.Conv2D(channels * (downsampleFactor ** 2), 3, padding=\"same\",\n",
    "#                activation=\"relu\", kernel_initializer=\"Orthogonal\")(x)\n",
    "#         x = layers.BatchNormalization(momentum=momentum)(x)\n",
    "        outputs = tf.nn.depth_to_space(x, downsampleFactor)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def upscale_block(block_input, filters=64, scale=1):\n",
    "# def upscale_block(block_input, filters=128, scale=1):\n",
    "#     x = layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn)(block_input)\n",
    "    x = tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, kernel_size=3, padding='same'))(block_input)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "    x = pixel_shuffle(x, filters, 1)\n",
    "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pixel_shuffle2(channels, downsampleFactor, momentum=0.8):\n",
    "    ups = tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2D(channels * (downsampleFactor * 2), 3, padding=\"same\",\n",
    "               activation=\"relu\", kernel_initializer=\"Orthogonal\"))\n",
    "    @tf.function\n",
    "    def fn(x):\n",
    "        x = ups(x)\n",
    "        return tf.nn.depth_to_space(x, downsampleFactor)\n",
    "    return fn\n",
    "\n",
    "def upscale_block2(filters=64, scale=1):\n",
    "    conv = tfa.layers.SpectralNormalization(\n",
    "            layers.Conv2D(filters, kernel_size=3, padding='same'))\n",
    "    shuffle = pixel_shuffle2(filters, scale)\n",
    "    act = layers.PReLU(shared_axes=[1, 2])\n",
    "    \n",
    "    @tf.function\n",
    "    def fn(block_input):\n",
    "    # def upscale_block(block_input, filters=128, scale=1):\n",
    "    #     x = layers.Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=init_fn)(block_input)\n",
    "        x = conv(block_input)\n",
    "    #     x = layers.BatchNormalization()(x)\n",
    "        x = shuffle(x)\n",
    "        x = act(x)\n",
    "        return x\n",
    "    return fn\n",
    "\n",
    "\n",
    "gen_vgg19 = model = tf.keras.Model(\n",
    "    inputs=vgg19.input,\n",
    "    outputs=vgg19.get_layer(\"block4_conv4\").output\n",
    ")\n",
    "model.trainable = False\n",
    "\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, filters, activation=False):\n",
    "    super(AttentionLayer, self).__init__()\n",
    "    self.filters = filters\n",
    "    self.activation = activation\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.in_shape = input_shape\n",
    "    b, hi, wi, c = input_shape\n",
    "    self.downscale1 = layers.Conv2D(tf.cast(c*4, tf.int32), 4, strides = 4)\n",
    "    self.downscale2 = layers.Conv2D(tf.cast(c*2, tf.int32), 2, strides = 2)\n",
    "    self.fc = self._conv(self.filters)\n",
    "    self.gc = self._conv(self.filters)\n",
    "    self.hc = self._conv(self.filters)\n",
    "    self.xc = self._conv(self.filters)\n",
    "#     self.upscale1 = upscale_block2(filters=self.filters, scale=4)\n",
    "#     self.upscale2 = upscale_block2(filters=self.filters, scale=2)\n",
    "    self.upscale1 = layers.UpSampling2D((4,4), interpolation='bilinear')\n",
    "    self.upscale2 = layers.UpSampling2D((2,2), interpolation='bilinear')\n",
    "\n",
    "    self.gamma = tf.Variable([1.], name=\"gamma\")\n",
    "    self.act = layers.LeakyReLU()\n",
    "#     self.downsized_dims = tf.cast(hi/6, tf.int32), tf.cast(wi/6, tf.int32), c*6\n",
    "    \n",
    "  def _conv(self, filters, kernel=1, strides=1):\n",
    "    return layers.Conv2D(filters, 1, strides=strides, padding=\"same\", use_bias=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "#   @tf.function\n",
    "  def call(self, inputs):\n",
    "    ch = self.filters\n",
    "    xdsh = self.in_shape\n",
    "    b = xdsh[0]\n",
    "    hi = xdsh[1]\n",
    "    wi = xdsh[2]\n",
    "    c = xdsh[3]\n",
    "        \n",
    "#     print(f\"shape:{[b, hi, wi, c]}\")\n",
    "    xd = self.downscale1(inputs)\n",
    "    xd = self.downscale2(xd)\n",
    "\n",
    "#     b, hi, wi, c = b, tf.cast(hi/6, tf.int32), tf.cast(wi/6, tf.int32), c*6\n",
    "    xdsh = tf.shape(xd)\n",
    "    b = xdsh[0]\n",
    "    hi = xdsh[1]\n",
    "    wi = xdsh[2]\n",
    "    c = xdsh[3]\n",
    "\n",
    "#     print(x.shape)\n",
    "    f = self.fc(xd) # [bs, h, w, c']\n",
    "    g = self.gc(xd) # [bs, h, w, c']\n",
    "    h = self.hc(xd) # [bs, h, w, c]\n",
    "    inputs = self.xc(inputs)\n",
    "    \n",
    "#     print(f\"f={f.shape}\")\n",
    "#     print(f\"g={g.shape}\")\n",
    "\n",
    "    f = tf.reshape(f, [-1, hi*wi, ch])\n",
    "    g = tf.reshape(g, [-1, hi*wi, ch])\n",
    "    h = tf.reshape(h, [-1, hi*wi, ch])\n",
    "#     print(f\"f={f.shape}\")\n",
    "#     print(f\"g={g.shape}\")\n",
    "#     print(f\"g={g.shape}, f={f.shape}\")\n",
    "\n",
    "    s = tf.matmul(g, f, transpose_b=True) # # [bs, N, N]\n",
    "\n",
    "    #     print(f\"s={s.shape}\")\n",
    "\n",
    "    beta = tf.nn.softmax(s)  # attention map\n",
    "\n",
    "#     print(f\"beta={beta.shape}, h={h.shape}\")\n",
    "\n",
    "    o = tf.matmul(beta, h) # [bs, N, C]\n",
    "\n",
    "#     print(f\"gamma={self.gamma}, o={o.shape} x={x.shape}\")\n",
    "    \n",
    "    o = tf.reshape(o, shape=[b, hi, wi, tf.cast(ch, tf.int32)])\n",
    "    o = self.upscale1(o)\n",
    "    o = self.upscale2(o)\n",
    "    \n",
    "#     print(f\"Reshaped o={o.shape}\")\n",
    "#     print(f\"gamma={self.gamma}, o={o.shape} inputs={inputs.shape}\")\n",
    "\n",
    "    x = self.gamma * o + inputs\n",
    "\n",
    "    if self.activation:\n",
    "        x = self.act(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# print(content_img.shape)\n",
    "# att = attention(content_img, 64)\n",
    "# display_image(att)\n",
    "def get_conv(filters):\n",
    "    conv = tf.keras.Sequential()\n",
    "    conv.add(tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, 3, padding=\"same\", use_bias=False)\n",
    "    ))\n",
    "    conv.add(layers.LeakyReLU())\n",
    "    return conv\n",
    "\n",
    "def get_encoder():\n",
    "    encoder = tf.keras.Sequential(name=\"encoder\")\n",
    "    encoder.add(get_conv(64))\n",
    "    encoder.add(get_conv(128))\n",
    "    encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "    encoder.add(get_conv(256))\n",
    "    encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "    encoder.add(get_conv(512))\n",
    "    encoder.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "    encoder.add(get_conv(1024))\n",
    "    encoder.add(layers.MaxPooling2D(pool_size = (3, 3), padding='same'))\n",
    "    return encoder\n",
    "\n",
    "def get_decoder():\n",
    "    decoder = tf.keras.Sequential(name=\"decoder\")\n",
    "    decoder.add(layers.UpSampling2D((3, 3), interpolation='bilinear'))\n",
    "    decoder.add(get_conv(512))\n",
    "    decoder.add(layers.UpSampling2D((2, 2), interpolation='bilinear'))\n",
    "    decoder.add(get_conv(256))\n",
    "    decoder.add(layers.UpSampling2D((2, 2), interpolation='bilinear'))\n",
    "    decoder.add(get_conv(128))\n",
    "    decoder.add(layers.UpSampling2D((2, 2), interpolation='bilinear'))\n",
    "    decoder.add(get_conv(64))\n",
    "    return decoder\n",
    "    \n",
    "\n",
    "class AutoEncoder(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    \n",
    "  def build(self, input_shape):\n",
    "    self.encoder = get_encoder()\n",
    "    self.decoder = get_decoder()\n",
    "    \n",
    "    self.encoder.build(input_shape=input_shape)\n",
    "    \n",
    "    sh = self.encoder.output_shape\n",
    "    \n",
    "    self.flatten = layers.Flatten()\n",
    "    self.seq1 = layers.Dense(1024)\n",
    "    self.reshape = layers.Reshape([sh[1], sh[2], sh[3]])\n",
    "    \n",
    "    self.last = layers.Conv2D(3, 3, padding='same', activation='tanh')\n",
    "    self.inputs_dropout = layers.Dropout(0.2)\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x = self.inputs_dropout(inputs)\n",
    "    x = self.encoder(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.reshape(x)\n",
    "    x = self.decoder(x)\n",
    "    \n",
    "    # x_inputs = self.conv_input(inputs)\n",
    "    \n",
    "    # x = self.add([x * self.gamma, x_inputs])\n",
    "    # x = self.attention(x)\n",
    "    x = self.last(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class Generator(tf.keras.Model):\n",
    "  def __init__(self, autoencoder):\n",
    "    super(Generator, self).__init__()\n",
    "    self.conv_input = get_conv(64)\n",
    "    self.conv_auto = get_conv(64)\n",
    "    self.gamma = tf.Variable([1.], name=\"gamma\")\n",
    "    self.add = layers.Add()\n",
    "    self.last = layers.Conv2D(3, 3, padding='same', activation='tanh')\n",
    "    self.attention = AttentionLayer(64, activation=True)\n",
    "    self.autoencoder = autoencoder\n",
    "    self.last = layers.Conv2D(3, 3, padding='same', activation='tanh')\n",
    "\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x_inputs = self.conv_input(inputs)\n",
    "    x = self.autoencoder(inputs)\n",
    "\n",
    "    x = self.conv_auto(x)\n",
    "    \n",
    "    x = self.add([x * self.gamma, x_inputs])\n",
    "    x = self.attention(x)\n",
    "    return self.last(x)\n",
    "\n",
    "def make_sgenerator_model(scale=8, num_filters=64, autoencoder=None):\n",
    "    return Generator(autoencoder)\n",
    "#     inputs = tf.keras.Input(shape=(None, None, 3))\n",
    "#     x = encoder(inputs)\n",
    "    \n",
    "\n",
    "\n",
    "#       model.add(AttentionLayer(64, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(AttentionLayer(128, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "#     model.add(AttentionLayer(256, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "#     model.add(AttentionLayer(512, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "#     model.add(AttentionLayer(1024, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.MaxPooling2D(pool_size = (2, 2), padding='same'))\n",
    "#     model.add(layers.UpSampling2D((2, 2)))\n",
    "#     model.add(AttentionLayer(512, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.UpSampling2D((2, 2)))\n",
    "#     model.add(AttentionLayer(256, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.UpSampling2D((2, 2)))\n",
    "#     model.add(AttentionLayer(128, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "#     model.add(layers.UpSampling2D((2, 2)))\n",
    "#     model.add(AttentionLayer(64, 3, padding=\"same\", activation='leaky_relu', use_bias=False))\n",
    "# #     model.add(AttentionLayer(128))\n",
    "#     model.add(layers.Conv2D(3, 3, padding='same', activation='tanh'))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     model.add(AttentionLayer(64))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(AttentionLayer(32))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(AttentionLayer(16))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(AttentionLayer(8))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(AttentionLayer(4))\n",
    "#     model.add(layers.LeakyReLU())\n",
    "#     model.add(layers.Conv2D(3, kernel_size=9, padding='same', activation='tanh'))\n",
    "\n",
    "#     x = AttentionLayer(64)(inputs)\n",
    "#     x = layers.LeakyReLU()(x)\n",
    "#     x = AttentionLayer(32)(x)\n",
    "#     x = layers.LeakyReLU()(x)\n",
    "    \n",
    "# #     y1 = residual_block(x, 32)\n",
    "#     x = AttentionLayer(16)(x)\n",
    "# #     x = layers.Add([x,y1])\n",
    "    \n",
    "    \n",
    "#     x = layers.LeakyReLU()(x)\n",
    "#     x = AttentionLayer(8)(x)\n",
    "#     x = layers.LeakyReLU()(x)\n",
    "#     x = AttentionLayer(4)(x)\n",
    "# #     x = layers.Conv2D(8, kernel_size=3, padding='same', activation='leaky_relu')(x)\n",
    "#     x = layers.Conv2D(3, kernel_size=9, padding='same', activation='tanh')(x)\n",
    "# #     x = tf.keras.activations.tanh(x)\n",
    "#     x = layers.Lambda(denormalize)(x)\n",
    "\n",
    "#     return model\n",
    "\n",
    "# test_model = make_sgenerator_model()\n",
    "\n",
    "# test_model.build(input_shape=[2, input_h, input_w, 64])\n",
    "\n",
    "# prepa_dir = pathlib.Path(\"./prepa\")\n",
    "# prepa_files = list(prepa_dir.glob('*.png'))\n",
    "\n",
    "# # # i = 0\n",
    "# # # ch = 16\n",
    "# content_img = load_and_process_image(prepa_files[0])\n",
    "# x = content_img\n",
    "\n",
    "# # layers.Conv2D(ch, 1, strides=1, padding=\"same\", use_bias=False)(x)\n",
    "# generator = make_sgenerator_model()\n",
    "# generator(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disc_bloc(filters, strides, name):\n",
    "    model = tf.keras.Sequential(name=name)\n",
    "    model.add(tfa.layers.SpectralNormalization(\n",
    "        layers.Conv2D(filters, kernel_size=3, strides=strides, padding='same')))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    return model\n",
    "\n",
    "\n",
    "# vgg19.get_layer(\"block4_conv4\").output\n",
    "disc_vgg19 = model = tf.keras.Model(\n",
    "    inputs=vgg19.get_layer(\"block4_pool\").input,\n",
    "    outputs=vgg19.output\n",
    ")\n",
    "model.trainable = False\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential(name=\"discriminator\")\n",
    "    # tf.cast(image * 127.5 + 127.5, np.uint8)\n",
    "#     model.add(layers.Lambda(lambda x: preprocess_input(x)))\n",
    "#     model.add(gen_vgg19)\n",
    "#     model.add(disc_vgg19)\n",
    "#     model.add(layers.GlobalAveragePooling2D())\n",
    "#     model.add(layers.Dense(1))\n",
    "\n",
    "#     model = tf.keras.Sequential()\n",
    "    filters = 64\n",
    "    model.add(\n",
    "        layers.Conv2D(filters, kernel_size=3, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    \n",
    "    model.add(disc_bloc(filters, 2, name=\"disc_bloc_1\"))\n",
    "#     model.add(disc_bloc(filters*2, 1))\n",
    "    model.add(disc_bloc(filters*2, 2, name=\"disc_bloc_2\"))\n",
    "#     model.add(disc_bloc(filters*4, 1))\n",
    "    model.add(disc_bloc(filters*4, 2, name=\"disc_bloc_3\"))\n",
    "#     model.add(disc_bloc(filters*8, 1))\n",
    "    model.add(disc_bloc(filters*8, 2, name=\"disc_bloc_4\"))\n",
    "    model.add(layers.Dense(1024))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = make_discriminator_model()\n",
    "# discriminator_downsized = make_discriminator_model()\n",
    "autoencoder = AutoEncoder()\n",
    "generator = make_sgenerator_model(autoencoder=autoencoder)\n",
    "\n",
    "# discriminator.build(input_shape=[2, input_h, input_w, 3])\n",
    "# print(discriminator.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = test_generator(generator, discriminator, autoencoder, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method returns a helper function to compute cross entropy loss\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def discriminator_downsized_loss(fake_output, is_downsized=True):\n",
    "#     if is_downsized:\n",
    "#         return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "#     else:\n",
    "#         return cross_entropy(tf.zeros_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "def autoencoder_loss(real_output, fake_output):\n",
    "    return mse(real_output, fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generator_downsized_loss(fake_output, is_downsized=True):\n",
    "#     if is_downsized:\n",
    "#         return cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "#     else:\n",
    "#         return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "noise_dim = 100\n",
    "num_examples_to_generate = 16\n",
    "\n",
    "# You will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "seed = tf.random.normal([num_examples_to_generate, noise_dim])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = f'logs/gan/train/{current_time}'\n",
    "train_log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(generator, discriminator, autoencoder, epoch, save=True): \n",
    "    predictions = test_generator(generator, discriminator, autoencoder, 3)\n",
    "    if save == True:\n",
    "        plt.savefig('./gan_output/image_at_epoch_{:04d}.png'.format(epoch))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# generate_and_save_images(generator, discriminator, 0, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import signal\n",
    "# import sys\n",
    "\n",
    "# def sigint_handler(signal, frame):\n",
    "#     print ('KeyboardInterrupt is caught')\n",
    "#     checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "#     sys.exit(0)\n",
    "# signal.signal(signal.SIGINT, sigint_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lrgen=tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[1000, 10000], values=[0.1, 1e-4, 1e-5])\n",
    "# lrdist=tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=[1000, 10000], values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "auto_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.00004, beta_1=0.5)\n",
    "# discriminator_down_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0004, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './gan4_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "#                                  discriminator_down_optimizer=discriminator_down_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator,\n",
    "#                                 discriminator_downsized=discriminator_downsized\n",
    "                                )\n",
    "ckpt_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     if ckpt_manager.latest_checkpoint:\n",
    "#         checkpoint.restore(ckpt_manager.latest_checkpoint)\n",
    "# except:\n",
    "#     print(\"Could not restore the checkopint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpus = tf.config.list_logical_devices('GPU')\n",
    "\n",
    "# with tf.device(gpus[0].name):\n",
    "\n",
    "class GANModel(tf.keras.Model):\n",
    "  def __init__(self, gen, disc):\n",
    "    super(GANModel, self).__init__(name=\"GANModel\")\n",
    "    self.generator = gen\n",
    "    self.discriminator = disc\n",
    "\n",
    "  def train_step(self, all_images):\n",
    "    images, resized = all_images\n",
    "    with tf.GradientTape() as auto_tape, tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        encoded = autoencoder(images, training=True)\n",
    "        auto_loss = autoencoder_loss(images, encoded)\n",
    "    \n",
    "        generated_images = generator(resized, training=True)\n",
    "    \n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        gen_loss = generator_loss(fake_output)\n",
    "        _gen_loss = gen_loss\n",
    "        mse = content_loss(images, generated_images)\n",
    "        gen_loss+=mse\n",
    "        \n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "#         images = tf.cast(images, tf.float32)\n",
    "        #         # avoid too large errors\n",
    "#         real_output = tf.clip_by_value(real_output, 0.0, 255.0)\n",
    "#         fake_output = tf.clip_by_value(fake_output, 0.0, 255.0)\n",
    "        \n",
    "#         dimages = denormalize(images)\n",
    "# #         dresized = denormalize(resized)\n",
    "#         dgenerated_images = denormalize(generated_images)\n",
    "#         ssim = tf.math.reduce_sum(tf.image.ssim(dimages, dgenerated_images, 255.0))\n",
    "#         ms_ssim = tf.math.reduce_sum(tf.image.ssim_multiscale(dimages, dgenerated_images, 255.0))\n",
    "#         same_ms_ssim = tf.math.reduce_sum(tf.image.ssim_multiscale(dimages, dimages, 255.0))\n",
    "#         resized_ms_ssim = tf.math.reduce_sum(tf.image.ssim_multiscale(dimages, dresized, 255.0))\n",
    "        \n",
    "#         downsized_output = discriminator_downsized(generated_images, training=True)\n",
    "#         disc_loss_down = tf.cond(same_ms_ssim == resized_ms_ssim,\n",
    "#                                     lambda: discriminator_downsized_loss(downsized_output, True),\n",
    "#                                  lambda: discriminator_downsized_loss(downsized_output, False))\n",
    "#         gen_loss += tf.cond(same_ms_ssim == resized_ms_ssim,\n",
    "#                                     lambda: generator_downsized_loss(downsized_output, True),\n",
    "#                                  lambda: generator_downsized_loss(downsized_output, False))\n",
    "\n",
    "#         ssim_loss =  1 * (same_ms_ssim - ssim)\n",
    "#         ms_ssim_loss = 1 *  (same_ms_ssim - ms_ssim)\n",
    "#         ssim_losses =  ssim_loss + ms_ssim_loss\n",
    "#         gen_loss += ssim_losses\n",
    "    \n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    gradients_of_autoencoder = auto_tape.gradient(auto_loss, autoencoder.trainable_variables)\n",
    "# #     gradients_of_discriminator_down = disc_tape2.gradient(disc_loss_down, discriminator_downsized.trainable_variables)\n",
    "    \n",
    "#     gradients_of_generator, _ = tf.clip_by_global_norm(gradients_of_generator, 5.0)\n",
    "#     gradients_of_discriminator, _ = tf.clip_by_global_norm(gradients_of_discriminator, 5.0)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    auto_optimizer.apply_gradients(zip(gradients_of_autoencoder, autoencoder.trainable_variables))\n",
    "#     discriminator_down_optimizer.apply_gradients(zip(gradients_of_discriminator_down, discriminator_downsized.trainable_variables))\n",
    "\n",
    "    return {\"gen_loss\": gen_loss, \"disc_loss\":disc_loss, \"auto_loss\":auto_loss\n",
    "#             \"_gen_loss\": _gen_loss,\n",
    "#             \"mse\":mse,\n",
    "#             \"ssim_loss\":ssim_loss, \"ms_ssim_loss\":ms_ssim_loss, \n",
    "#             \"ssim\":ssim, \"ms_ssim\":ms_ssim, \"same_ms_ssim\":same_ms_ssim\n",
    "           }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_epoch_end(epoch, logs=None):\n",
    "    pass\n",
    "#     clear_output(wait=True)\n",
    "#     generate_and_save_images(generator, discriminator,\n",
    "#                     epoch + 1)\n",
    "#     ckpt_save_path = ckpt_manager.save()\n",
    "#     print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "#                                                                 ckpt_save_path))\n",
    "def on_batch_end(batch, logs):\n",
    "    if batch % 99 == 0 and batch > 0:\n",
    "        if \"gen_loss\" in logs:\n",
    "            \n",
    "            tf.summary.scalar('batch_gen_loss', tf.constant(logs[\"gen_loss\"], tf.float16), step=batch*BATCH_SIZE)\n",
    "            tf.summary.scalar('batch_disc_loss', tf.constant(logs[\"disc_loss\"], tf.float16), step=batch*BATCH_SIZE)\n",
    "            # tf.summary.scalar('ms_ssim', logs[\"ms_ssim\"], step=batch*BATCH_SIZE)\n",
    "        clear_output(wait=True)\n",
    "        print(\"Logs:\")\n",
    "        print(logs)\n",
    "        generate_and_save_images(generator, discriminator, autoencoder,\n",
    "                        batch + 1)\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('Saving checkpoint for batch {} at {}'.format(batch+1,\n",
    "                                                                    ckpt_save_path))\n",
    "                                                        \n",
    "lm = tf.keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end, on_batch_end=on_batch_end)\n",
    "\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = train_log_dir,\n",
    "      write_graph=True, # visualize the graph\n",
    "     histogram_freq = 1, update_freq=100,\n",
    "#      profile_batch = (1,200)\n",
    "                                                )\n",
    "\n",
    "# train(train_ds, EPOCHS)\n",
    "model = GANModel(generator, discriminator)\n",
    "model.compile(metrics=[\"gen_loss\", \"disc_loss\"])\n",
    "# model.compile(metrics=[\"gen_loss\", \"disc_loss\", \"ms_ssim\", \"ms\"], run_eagerly=True)\n",
    "model.fit(\n",
    "    train_ds.repeat(),\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[tboard_callback, lm],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "#     steps_per_epoch=int(image_count/BATCH_SIZE),\n",
    "    workers=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_model(model, new_input_shape=(None, 40, 40, 3)):\n",
    "    # replace input shape of first layer\n",
    "    # model.layers[1].batch_input_shape = new_input_shape\n",
    "    # input_layer = layers.InputLayer(input_shape=new_input_shape, name=\"input_1\")\n",
    "    # model.input = input_layer\n",
    "\n",
    "    new_model = make_sgenerator_model(new_input_shape)\n",
    "\n",
    "    # feel free to modify additional parameters of other layers, for example...\n",
    "    # model._layers[2].pool_size = (8, 8)\n",
    "    # model._layers[2].strides = (8, 8)\n",
    "\n",
    "    # rebuild model architecture by exporting and importing via json\n",
    "    # new_model = keras.models.model_from_json(model.to_json())\n",
    "    new_model.summary()\n",
    "\n",
    "    # copy weights from old model to new one\n",
    "    for layer in new_model.layers:\n",
    "        try:\n",
    "            layer.set_weights(model.get_layer(name=layer.name).get_weights())\n",
    "        except:\n",
    "            print(\"Could not transfer weights for layer {}\".format(layer.name))\n",
    "\n",
    "    # test new model on a random input image\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = checkpoint_dir+\"/weights/weights-{epoch:04d}.ckpt\"\n",
    "\n",
    "generator.save_weights(checkpoint_path.format(epoch=0))\n",
    "discriminator.save_weights(checkpoint_path.format(epoch=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./weights/gan/2/{}/\"\n",
    "\n",
    "gen_path = os.path.join(save_path.format(\"generator\"))\n",
    "tf.saved_model.save(generator, gen_path)\n",
    "disc_path = os.path.join(save_path.format(\"discriminator\"))\n",
    "tf.saved_model.save(discriminator, disc_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_generator = tf.saved_model.load(gen_path)\n",
    "loaded_discriminator = tf.saved_model.load(disc_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_save_images(loaded_generator, loaded_discriminator,\n",
    "                             9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4327198819dafd55a2243f22aba11bf2a7d9f0c32aced8ba7d18a900e49d0553"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
